project: "gcp-project-id"
location: "global"
bucket: "mes-audio-bucket"

write_to_bigquery: true

vertexai:
  project_id: "gcp-project-id"
  location: "europe-west2"
  model_armour_template_id: "default"

bigquery:
  enabled: true
  project_id: "gcp-project-id"
  location: "europe-west2"
  dataset_name: "llm_experiments"
  table_name: "experiment_results"

gcs_files:
  - "calls/claim_*.wav"
  - "calls/followup_*.wav"

experiments:
  - name: "summarisation_baseline"
    use_case: "summarisation"
    client:
      name: "gemini"
      model_id: "gemini-1.5-pro"
    generation_config:
      temperature: 0.2
      top_k: 40
      top_p: 0.8
      max_output_tokens: 1024
    prompt_id: "summarisation-prompt-v1"
    metrics:
      - "vertexai_groundedness"
      - "vertexai_fluency"
      - "vertexai_summarization_quality"
      - "summarization_quality"
    
  - name: "summarisation_flash"
    use_case: "summarisation"
    client:
      name: "gemini"
      model_id: "gemini-1.5-flash"
    generation_config:
      temperature: 0.2
      top_k: 40
      top_p: 0.8
      max_output_tokens: 1024
    prompt_id: "summarisation-prompt-v1"
    metrics:
      - "vertexai_groundedness"
      - "vertexai_fluency"
      - "vertexai_summarization_quality"
      - "summarisation_quality"

reference_generation:
  client:
    name: "gemini"
    model_id: "gemini-1.5-pro"
  prompt_id: "transcription-prompt-v1"
  generation_config:
    temperature: 0.2
    top_k: 40
    top_p: 0.8
    max_output_tokens: 1024

reference_config:
  client:
    model_id: "gemini-1.5-pro"
  prompt_id: "transcription-prompt-v1"
  generation_config:
    temperature: 0.2
    top_k: 40
    top_p: 0.8
    max_output_tokens: 1024

concurrency:
  max_workers: 4
  batch_size: 8

retry:
  max_attempts: 3
  backoff_factor: 2.0
  max_wait_time: 60.0