{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MES Demo: From Experiment to Production Analysis\n",
    "\n",
    "This notebook tells the story of how a Data Scientist uses the **Model Evaluation Suite (MES)** to build and analyse a Gen AI agent for summarising insurance calls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Goal\n",
    "Our goal is to create a Gen AI agent that can accurately summarize insurance claim calls. We'll use MES to make data-driven decisions at two key stages:\n",
    "1.  **Pre-Agent Development:** Choosing the best prompt to build our agent.\n",
    "2.  **Post-Agent Development:** Analyzing the agent's responses for safety and reliability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## Part 1: Pre-Agent - Choosing the Right Prompt\n",
    "Before we build an agent, we need to select the best components. The prompt is one of the most critical. A good prompt is the difference between a useful response and a useless one.\n",
    "\n",
    "Using MES, we've set up two experiments to compare a **basic prompt** with a **highly-engineered 'good' prompt**. Let's run them and see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "sys.path.append('../src')\n",
    "from orchestrator.experiment_runner import ExperimentRunner\n",
    "\n",
    "# Initialise the runner with our demo configuration\n",
    "print('Initialising MES with demo configuration...')\n",
    "config_path = '../config/demo_experiments.yaml'\n",
    "runner = ExperimentRunner(config_path)\n",
    "\n",
    "# Run the experiments\n",
    "results_df = runner.run_experiments()\n",
    "print('\\n✅ Experiments complete! Analysing results...')\n",
    "\n",
    "# Display the raw results dataframe\n",
    "display(results_df[['experiment_name', 'model_id', 'response', 'vertexai_fluency', 'vertexai_coherence', 'processing_time']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Side-by-Side Comparison\n",
    "The power of MES is making these comparisons easy. Let's look at the generated summaries from both prompts for the same call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_prompt_response = results_df[results_df['experiment_name'] == 'summarisation_good_prompt']['response'].iloc[0]\n",
    "basic_prompt_response = results_df[results_df['experiment_name'] == 'summarisation_basic_prompt']['response'].iloc[0]\n",
    "\n",
    "display(Markdown('### Good Prompt Response (Structured)'))\n",
    "display(Markdown(good_prompt_response))\n",
    "\n",
    "display(Markdown('### Basic Prompt Response (Unstructured)'))\n",
    "display(Markdown(basic_prompt_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data-Driven Decision Making\n",
    "Visually, the 'good' prompt is clearly better. But MES provides the quantitative data to back this up. Let's compare the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_agent_metrics = [\n",
    "    'vertexai_fluency',\n",
    "    'vertexai_coherence',\n",
    "    'vertexai_verbosity',\n",
    "    'input_tokens',\n",
    "    'output_tokens',\n",
    "    'processing_time'\n",
    "]\n",
    "\n",
    "comparison_df = results_df.groupby('experiment_name')[pre_agent_metrics].mean().round(2)\n",
    "display(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Pre-Agent Conclusion:**\n",
    "As a Data Scientist, the story is clear. The `summarisation_good_prompt` experiment produced a well-structured, fluent, and coherent summary. The metrics from MES confirm this, showing higher scores across the board.\n",
    "\n",
    "**MES allowed me to quickly and systematically prove which prompt is superior. I can now confidently select the 'good' prompt to build our production agent.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Post-Agent - Downstream Analysis\n",
    "Now that we've selected our best prompt and hypothetically built our agent, the job isn't over. We need to continuously analyse its output to ensure it's not only accurate but also safe and trustworthy.\n",
    "\n",
    "This is where MES provides value for **downstream analysis**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing for Hallucinations and Safety\n",
    "A major risk with Gen AI is **hallucination**—making things up. For our use case, a summary that invents details would be disastrous. We also need to ensure the output is **safe** and free of harmful content.\n",
    "\n",
    "MES helps us measure this. Let's look at the `GROUNDEDNESS` and `SAFETY` metrics for the response from our chosen 'good' prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for the results of our chosen 'best' experiment\n",
    "best_result = results_df[results_df['experiment_name'] == 'summarisation_good_prompt'].iloc[0]\n",
    "\n",
    "# Display the generated summary alongside its source transcript ('reference')\n",
    "display(Markdown('### Generated Summary'))\n",
    "display(Markdown(best_result['response']))\n",
    "display(Markdown('### Original Transcript (Reference for Groundedness)'))\n",
    "display(Markdown(f\"```\\n{best_result['reference']}\\n```\"))\n",
    "\n",
    "# Show the post-agent analysis metrics\n",
    "post_agent_metrics = {\n",
    "    'Groundedness': best_result['vertexai_groundedness'],\n",
    "    'Safety': best_result['vertexai_safety'],\n",
    "    'Summarization Quality': best_result['vertexai_summarization_quality']\n",
    "}\n",
    "post_agent_df = pd.DataFrame.from_dict(post_agent_metrics, orient='index', columns=['Score'])\n",
    "\n",
    "display(Markdown('### Post-Agent Analysis Scores'))\n",
    "display(post_agent_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Post-Agent Conclusion:**\n",
    "The high `Groundedness` score tells us the summary is factually based on the transcript, minimizing the risk of hallucination. The high `Safety` score confirms the content is appropriate.\n",
    "\n",
    "**With MES, a Data Scientist can perform this critical downstream analysis to ensure the Gen AI agent remains reliable and trustworthy after deployment.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Demo Summary\n",
    "Today, we've seen how the **Model Evaluation Suite** empowers our Data Scientists throughout the entire lifecycle of a Gen AI agent:\n",
    "\n",
    "1.  **Before development**, it provides a data-driven way to experiment and select the best components, like prompts and models.\n",
    "2.  **After development**, it provides the tools for crucial downstream analysis, ensuring our agents are safe, grounded, and reliable.\n",
    "\n",
    "Ultimately, MES helps us build **better, safer models, faster**, reducing risk and accelerating our time-to-value with Generative AI."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
