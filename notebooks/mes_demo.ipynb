{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MES Demo: From Experiment to Production Analysis\n",
    "\n",
    "This notebook tells the story of how a Data Scientist uses the **Model Evaluation Suite (MES)** to build and analyze a Gen AI agent for summarising insurance calls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Goal\n",
    "Our goal is to create a Gen AI agent that can accurately summarise insurance claim calls. We'll use MES to make data-driven decisions at two key stages:\n",
    "1.  **Pre-Agent Development:** Choosing the best prompt to build our agent.\n",
    "2.  **Post-Agent Development:** Analyzing the agent's responses for safety and reliability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## Part 1: Pre-Agent - Choosing the Right Prompt\n",
    "Before we build an agent, we need to select the best components. The prompt is one of the most critical. A good prompt is the difference between a useful response and a useless one.\n",
    "\n",
    "Using MES, we've set up two experiments to compare a **basic prompt** with a **highly-engineered 'advanced' prompt**. Let's look at the prompts first, then run the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Add src directory to path and import the runner\n",
    "sys.path.append('../src')\n",
    "from orchestrator.experiment_runner import ExperimentRunner\n",
    "from utils.prompt_manager import PromptManager\n",
    "\n",
    "# Initialize the runner and prompt manager\n",
    "config_path = '../config/demo_experiments.yaml'\n",
    "runner = ExperimentRunner(config_path)\n",
    "prompt_manager = PromptManager(runner.config['project'], runner.config['location'])\n",
    "\n",
    "# Load prompts from Vertex AI Prompt Management\n",
    "advanced_prompt_id = runner.config['experiments'][0]['prompt_id']\n",
    "basic_prompt_id = runner.config['experiments'][1]['prompt_id']\n",
    "\n",
    "advanced_prompt_text = prompt_manager.load(advanced_prompt_id)\n",
    "basic_prompt_text = prompt_manager.load(basic_prompt_id)\n",
    "\n",
    "display(Markdown('### Advanced Prompt'))\n",
    "display(Markdown(f'```markdown\\n{advanced_prompt_text}\\n```'))\n",
    "\n",
    "display(Markdown('### Basic Prompt'))\n",
    "display(Markdown(f'```markdown\\n{basic_prompt_text}\\n```'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the experiments\n",
    "print('\\nRunning experiments... This will take a moment.')\n",
    "results_df = runner.run_experiments()\n",
    "print('\\n✅ Experiments complete! Analyzing results...')\n",
    "\n",
    "# Display the raw results dataframe with the new summarisation_quality scores\n",
    "display(results_df[['experiment_name', 'model_id', 'response', 'summarisation_section_coverage', 'summarisation_required_sections', 'vertexai_fluency', 'vertexai_coherence', 'processing_time']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Side-by-Side Comparison\n",
    "The power of MES is making these comparisons easy. Let's look at the generated summaries from both prompts for the same call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_prompt_response = results_df[results_df['experiment_name'] == 'summarisation_good_prompt']['response'].iloc[0]\n",
    "basic_prompt_response = results_df[results_df['experiment_name'] == 'summarisation_basic_prompt']['response'].iloc[0]\n",
    "\n",
    "display(Markdown('### Advanced Prompt Response (Structured)'))\n",
    "display(Markdown(good_prompt_response))\n",
    "\n",
    "display(Markdown('### Basic Prompt Response (Unstructured)'))\n",
    "display(Markdown(basic_prompt_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data-Driven Decision Making\n",
    "Visually, the 'advanced' prompt is clearly better. But MES provides the quantitative data to back this up. Let's compare the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_agent_metrics = [\n",
    "    'vertexai_fluency',\n",
    "    'vertexai_coherence',\n",
    "    'vertexai_verbosity',\n",
    "    'input_tokens',\n",
    "    'output_tokens',\n",
    "    'processing_time'\n",
    "]\n",
    "\n",
    "comparison_df = results_df.groupby('experiment_name')[pre_agent_metrics].mean().round(2)\n",
    "display(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Pre-Agent Conclusion:**\n",
    "As a Data Scientist, the story is clear. The `summarisation_good_prompt` experiment produced a well-structured, fluent, and coherent summary. The metrics from MES confirm this, showing higher scores across the board.\n",
    "\n",
    "**MES allowed me to quickly and systematically prove which prompt is superior. I can now confidently select the 'advanced' prompt to build our production agent.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Post-Agent - Downstream Analysis\n",
    "Now that we've selected our best prompt and hypothetically built our agent, the job isn't over. We need to continuously analyze its output to ensure it's not only accurate but also safe and trustworthy.\n",
    "\n",
    "This is where MES provides value for **downstream analysis**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing for Hallucinations and Safety\n",
    "A major risk with Gen AI is **hallucination**—making things up. For our insurance use case, a summary that invents details would be disastrous. We also need to ensure the output is **safe** and free of harmful content.\n",
    "\n",
    "MES helps us measure this. Let's look at the `GROUNDEDNESS` and `SAFETY` metrics for the response from our chosen 'advanced' prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for the results of our chosen 'best' experiment\n",
    "best_result = results_df[results_df['experiment_name'] == 'summarisation_good_prompt'].iloc[0]\n",
    "\n",
    "# Display the generated summary alongside its source transcript ('reference')\n",
    "display(Markdown('### Generated Summary'))\n",
    "display(Markdown(best_result['response']))\n",
    "display(Markdown('### Original Transcript (Reference for Groundedness)'))\n",
    "display(Markdown(f\"```\\n{best_result['reference']}\\n```\"))\n",
    "\n",
    "# Show the post-agent analysis metrics\n",
    "post_agent_metrics = {\n",
    "    'Groundedness': best_result['vertexai_groundedness'],\n",
    "    'Safety': best_result['vertexai_safety'],\n",
    "    'Summarization Quality': best_result['vertexai_summarization_quality']\n",
    "}\n",
    "post_agent_df = pd.DataFrame.from_dict(post_agent_metrics, orient='index', columns=['Score'])\n",
    "\n",
    "display(Markdown('### Post-Agent Analysis Scores'))\n",
    "display(post_agent_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Post-Agent Conclusion:**\n",
    "The high `Groundedness` score tells us the summary is factually based on the transcript, minimizing the risk of hallucination. The high `Safety` score confirms the content is appropriate.\n",
    "\n",
    "**With MES, a Data Scientist can perform this critical downstream analysis to ensure the Gen AI agent remains reliable and trustworthy after deployment.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Demo Summary\n",
    "Today, we've seen how the **Model Evaluation Suite** empowers our Data Scientists throughout the entire lifecycle of a Gen AI agent:\n",
    "\n",
    "1.  **Before development**, it provides a data-driven way to experiment and select the best components, like prompts and models.\n",
    "2.  **After development**, it provides the tools for crucial downstream analysis, ensuring our agents are safe, grounded, and reliable.\n",
    "\n",
    "Ultimately, MES helps us build **better, safer models, faster**, reducing risk and accelerating our time-to-value with Generative AI."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
