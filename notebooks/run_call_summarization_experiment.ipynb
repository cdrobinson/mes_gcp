{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29a73996",
   "metadata": {},
   "source": [
    "# Call Summarization Experiment\n",
    "\n",
    "This notebook runs the Call Summarization Experiment using multiple LLM configurations with advanced evaluation metrics including ROUGE and LLM-as-a-Judge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddad1e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from agents.call_summarization_agent import CallSummarizationAgent\n",
    "from orchestration.experiment_runner import ExperimentRunner\n",
    "from orchestration.bigquery_writer import BigQueryWriter\n",
    "from core.gcp_client import (\n",
    "    get_gcs_audio_bucket_name,\n",
    "    get_gcs_audio_dataset_paths,\n",
    "    get_bq_summarization_table_id,\n",
    "    config as main_global_config\n",
    ")\n",
    "\n",
    "def run_summarization_experiment():\n",
    "    print(\"--- Starting Call Summarization Experiment (Multi-LLM with Advanced Metrics) ---\")\n",
    "\n",
    "    # 1. Initialize Experiment Runner with Advanced Metrics\n",
    "    runner = ExperimentRunner(\n",
    "        agent_class=CallSummarizationAgent,\n",
    "        use_advanced_metrics=True,  # Enable ROUGE and LLM-as-a-Judge\n",
    "        judge_llm_config_name=\"gemini_1_5_flash_default\",  # Use different model for judging\n",
    "        include_llm_judge=True,\n",
    "        include_rouge=True\n",
    "    )\n",
    "    print(\"Initialized ExperimentRunner with advanced metrics (ROUGE + LLM-as-a-Judge).\")\n",
    "\n",
    "    # 2. Define Experiment Parameters\n",
    "    gcs_bucket = get_gcs_audio_bucket_name()\n",
    "    dataset_paths = get_gcs_audio_dataset_paths()\n",
    "    dataset_key = 'sample_summarization_set'\n",
    "    audio_folder_path = dataset_paths.get(dataset_key)\n",
    "\n",
    "    if not audio_folder_path or 'your-gcs-bucket' in gcs_bucket or 'path/to' in audio_folder_path:\n",
    "        print(f\"ERROR: Please configure GCS bucket and '{dataset_key}' path in configs/main_config.yaml.\")\n",
    "        return\n",
    "    print(f\"Targeting GCS: gs://{gcs_bucket}/{audio_folder_path}\")\n",
    "\n",
    "    # 3. Specify LLM configurations to test (names from main_config.yaml)\n",
    "    all_defined_llm_configs = list(main_global_config['gcp']['vertex_ai']['llm_configurations'].keys())\n",
    "    if not all_defined_llm_configs:\n",
    "        print(\"ERROR: No LLM configurations found in 'configs/main_config.yaml' under 'vertex_ai.llm_configurations'.\")\n",
    "        return\n",
    "        \n",
    "    # Test specific configurations with different parameters\n",
    "    llm_configs_to_test = [\"gemini_1_5_flash_default\", \"gemini_1_0_pro_concise\"]\n",
    "    llm_configs_to_test = [name for name in llm_configs_to_test if name in all_defined_llm_configs]\n",
    "    if not llm_configs_to_test:\n",
    "        print(f\"ERROR: None of the specified llm_configs_to_test were found in main_config.yaml. Available: {all_defined_llm_configs}\")\n",
    "        return\n",
    "\n",
    "    print(f\"LLM configurations to be tested: {llm_configs_to_test}\")\n",
    "\n",
    "    # 4. Optional: Global overrides for this experiment run\n",
    "    global_llm_params_override = {\n",
    "        \"temperature\": 0.15,\n",
    "        # \"max_output_tokens\": 200\n",
    "    }\n",
    "    print(f\"Global LLM parameter overrides: {global_llm_params_override}\")\n",
    "\n",
    "    global_prompt_override = None\n",
    "    global_system_prompt_override = None\n",
    "\n",
    "    # 5. Run the Experiment\n",
    "    print(f\"\\nStarting experiment with advanced evaluation metrics...\")\n",
    "    results = runner.run_experiment(\n",
    "        gcs_audio_folder_path=audio_folder_path,\n",
    "        gcs_bucket_name=gcs_bucket,\n",
    "        llm_config_names=llm_configs_to_test,\n",
    "        global_llm_parameters_override=global_llm_params_override,\n",
    "        global_prompt_override=global_prompt_override,\n",
    "        global_system_prompt_override=global_system_prompt_override\n",
    "    )\n",
    "\n",
    "    if not results:\n",
    "        print(\"Experiment finished but produced no results.\")\n",
    "        return\n",
    "    print(f\"\\nExperiment completed. Generated {len(results)} result entries.\")\n",
    "    \n",
    "    # Display sample results with new metrics\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(\"\\nSample of new evaluation metrics:\")\n",
    "    metric_columns = [col for col in results_df.columns if 'rouge' in col or 'llm_judge' in col]\n",
    "    if metric_columns:\n",
    "        print(results_df[metric_columns[:10]].head(2).to_string())\n",
    "    \n",
    "    print(\"\\nFirst 2 experiment result entries:\")\n",
    "    print(results_df.head(2)[['run_id', 'agent_type', 'llm_config_name_attempted']].to_string())\n",
    "\n",
    "    # 6. Write Results to BigQuery\n",
    "    try:\n",
    "        bq_writer = BigQueryWriter()\n",
    "        summarization_table_id = get_bq_summarization_table_id()\n",
    "        print(f\"\\nAttempting to write {len(results)} results to BigQuery table: {bq_writer.dataset_id}.{summarization_table_id}\")\n",
    "        bq_writer.write_results(results, summarization_table_id)\n",
    "        print(\"Successfully wrote results with advanced metrics to BigQuery.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing results to BigQuery: {e}\")\n",
    "\n",
    "    print(\"--- Call Summarization Experiment Finished ---\")\n",
    "\n",
    "# Run the experiment\n",
    "run_summarization_experiment()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
