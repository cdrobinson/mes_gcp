{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8efc10bd",
   "metadata": {},
   "source": [
    "# BigQuery Operations for LLM Experiment Results\n",
    "\n",
    "This notebook demonstrates how to use the BigQuery client to create datasets, tables, and manage experiment results.\n",
    "\n",
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda6ac13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from google.cloud import bigquery\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add src directory to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "from clients.bigquery_client import BigQueryClient\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"BigQuery Operations notebook initialised successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7064a4f",
   "metadata": {},
   "source": [
    "## Initialise BigQuery Client\n",
    "\n",
    "Set up the BigQuery client with your project configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71017efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - Update these values for your project\n",
    "PROJECT_ID = \"gcp-project-id\"\n",
    "LOCATION = \"europe-west2\"\n",
    "DATASET_NAME = \"mes-llm_experiments\"\n",
    "TABLE_NAME = \"experiment_results\"\n",
    "\n",
    "# Initialise BigQuery client\n",
    "bq_client = BigQueryClient(\n",
    "    project_id=PROJECT_ID,\n",
    "    location=LOCATION\n",
    ")\n",
    "\n",
    "print(f\"BigQuery client initialised for project: {PROJECT_ID}\")\n",
    "print(f\"Location: {LOCATION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b476350e",
   "metadata": {},
   "source": [
    "## Create Dataset and Table\n",
    "\n",
    "Create the dataset and table structure for storing experiment results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91b814c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "dataset = bq_client.create_dataset(DATASET_NAME, exists_ok=True)\n",
    "print(f\"Dataset created/verified: {dataset.dataset_id}\")\n",
    "\n",
    "# Define table schema for experiment results\n",
    "schema = [\n",
    "    bigquery.SchemaField(\"experiment_name\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"model_id\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"use_case\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"audio_file\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"response_text\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"metadata\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"input_tokens\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"output_tokens\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"total_tokens\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"processing_time\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"timestamp\", \"TIMESTAMP\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"run_timestamp\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"inserted_at\", \"TIMESTAMP\", mode=\"NULLABLE\"),\n",
    "    # Metric columns\n",
    "    bigquery.SchemaField(\"transcript_confidence\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"transcript_format_compliance\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"safety_overall\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"safety_toxicity\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"error\", \"STRING\", mode=\"NULLABLE\"),\n",
    "]\n",
    "\n",
    "# Create table with time partitioning and clustering\n",
    "time_partitioning = bigquery.TimePartitioning(\n",
    "    type_=bigquery.TimePartitioningType.DAY,\n",
    "    field=\"inserted_at\"\n",
    ")\n",
    "\n",
    "clustering_fields = [\"experiment_name\", \"model_id\", \"use_case\"]\n",
    "\n",
    "table = bq_client.create_table(\n",
    "    dataset_name=DATASET_NAME,\n",
    "    table_name=TABLE_NAME,\n",
    "    schema=schema,\n",
    "    exists_ok=True,\n",
    "    time_partitioning=time_partitioning,\n",
    "    clustering_fields=clustering_fields\n",
    ")\n",
    "\n",
    "print(f\"Table created/verified: {table.table_id}\")\n",
    "print(f\"Schema fields: {len(table.schema)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46265d8f",
   "metadata": {},
   "source": [
    "## Insert Sample Data\n",
    "\n",
    "Create and insert sample experiment data to test the BigQuery integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58f6d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample experiment data\n",
    "sample_data = {\n",
    "    'experiment_name': ['transcription_baseline', 'transcription_flash', 'transcription_baseline'],\n",
    "    'model_id': ['gemini-1.5-pro', 'gemini-1.5-flash', 'gemini-1.5-pro'],\n",
    "    'use_case': ['transcription', 'transcription', 'transcription'],\n",
    "    'audio_file': ['gs://bucket/audio1.wav', 'gs://bucket/audio2.wav', 'gs://bucket/audio3.wav'],\n",
    "    'response_text': ['Sample transcription 1', 'Sample transcription 2', 'Sample transcription 3'],\n",
    "    'metadata': ['{\"confidence\": 0.95}', '{\"confidence\": 0.87}', '{\"confidence\": 0.92}'],\n",
    "    'input_tokens': [150, 145, 148],\n",
    "    'output_tokens': [75, 82, 78],\n",
    "    'total_tokens': [225, 227, 226],\n",
    "    'processing_time': [2.34, 1.89, 2.12],\n",
    "    'timestamp': [datetime.now(), datetime.now(), datetime.now()],\n",
    "    'run_timestamp': ['20241201_143022', '20241201_143022', '20241201_143022'],\n",
    "    'inserted_at': [datetime.now(), datetime.now(), datetime.now()],\n",
    "    'transcript_confidence': [0.95, 0.87, 0.92],\n",
    "    'transcript_format_compliance': [1.0, 0.95, 1.0],\n",
    "    'safety_overall': [0.15, 0.12, 0.18],\n",
    "    'safety_toxicity': [0.05, 0.03, 0.07]\n",
    "}\n",
    "\n",
    "sample_df = pd.DataFrame(sample_data)\n",
    "print(f\"Created sample data with {len(sample_df)} rows\")\n",
    "display(sample_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb08f8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert sample data into BigQuery\n",
    "try:\n",
    "    bq_client.insert_rows_from_dataframe(\n",
    "        dataset_name=DATASET_NAME,\n",
    "        table_name=TABLE_NAME,\n",
    "        df=sample_df,\n",
    "        ignore_unknown_values=True,\n",
    "        skip_invalid_rows=False\n",
    "    )\n",
    "    print(f\"Successfully inserted {len(sample_df)} rows into BigQuery\")\n",
    "except Exception as e:\n",
    "    print(f\"Error inserting data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b439b95c",
   "metadata": {},
   "source": [
    "## Query Experiment Results\n",
    "\n",
    "Demonstrate querying experiment results from BigQuery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b91af8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query all experiment results\n",
    "query = f\"\"\"\n",
    "SELECT \n",
    "    experiment_name,\n",
    "    model_id,\n",
    "    COUNT(*) as total_runs,\n",
    "    AVG(transcript_confidence) as avg_confidence,\n",
    "    AVG(safety_overall) as avg_safety,\n",
    "    AVG(processing_time) as avg_processing_time,\n",
    "    SUM(total_tokens) as total_tokens_used\n",
    "FROM `{PROJECT_ID}.{DATASET_NAME}.{TABLE_NAME}`\n",
    "WHERE error IS NULL\n",
    "GROUP BY experiment_name, model_id\n",
    "ORDER BY avg_confidence DESC\n",
    "\"\"\"\n",
    "\n",
    "print(\"Querying experiment summary...\")\n",
    "results = bq_client.query(query)\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "query_df = pd.DataFrame([dict(row) for row in results])\n",
    "print(f\"Query returned {len(query_df)} rows\")\n",
    "display(query_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2fc3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query recent experiments\n",
    "recent_query = f\"\"\"\n",
    "SELECT \n",
    "    experiment_name,\n",
    "    model_id,\n",
    "    audio_file,\n",
    "    transcript_confidence,\n",
    "    safety_overall,\n",
    "    processing_time,\n",
    "    timestamp\n",
    "FROM `{PROJECT_ID}.{DATASET_NAME}.{TABLE_NAME}`\n",
    "WHERE inserted_at >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 1 DAY)\n",
    "ORDER BY timestamp DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "print(\"Querying recent experiments...\")\n",
    "recent_results = bq_client.query(recent_query)\n",
    "recent_df = pd.DataFrame([dict(row) for row in recent_results])\n",
    "\n",
    "if len(recent_df) > 0:\n",
    "    print(f\"Found {len(recent_df)} recent experiments\")\n",
    "    display(recent_df)\n",
    "else:\n",
    "    print(\"No recent experiments found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf5627e",
   "metadata": {},
   "source": [
    "## Visualize BigQuery Results\n",
    "\n",
    "Create visualizations from BigQuery data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc548ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model performance comparison\n",
    "if len(query_df) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Confidence by model\n",
    "    sns.barplot(data=query_df, x='model_id', y='avg_confidence', ax=axes[0,0])\n",
    "    axes[0,0].set_title('Average Transcript Confidence by Model')\n",
    "    axes[0,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Safety by model\n",
    "    sns.barplot(data=query_df, x='model_id', y='avg_safety', ax=axes[0,1])\n",
    "    axes[0,1].set_title('Average Safety Score by Model')\n",
    "    axes[0,1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Processing time by model\n",
    "    sns.barplot(data=query_df, x='model_id', y='avg_processing_time', ax=axes[1,0])\n",
    "    axes[1,0].set_title('Average Processing Time by Model')\n",
    "    axes[1,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Token usage by model\n",
    "    sns.barplot(data=query_df, x='model_id', y='total_tokens_used', ax=axes[1,1])\n",
    "    axes[1,1].set_title('Total Token Usage by Model')\n",
    "    axes[1,1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a64d906",
   "metadata": {},
   "source": [
    "## Advanced BigQuery Analytics\n",
    "\n",
    "Perform more complex analytics on experiment data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae13d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced analytics query\n",
    "analytics_query = f\"\"\"\n",
    "WITH experiment_stats AS (\n",
    "  SELECT \n",
    "    experiment_name,\n",
    "    model_id,\n",
    "    AVG(transcript_confidence) as avg_confidence,\n",
    "    STDDEV(transcript_confidence) as std_confidence,\n",
    "    AVG(safety_overall) as avg_safety,\n",
    "    AVG(processing_time) as avg_processing_time,\n",
    "    COUNT(*) as sample_count,\n",
    "    MIN(timestamp) as first_run,\n",
    "    MAX(timestamp) as last_run\n",
    "  FROM `{PROJECT_ID}.{DATASET_NAME}.{TABLE_NAME}`\n",
    "  WHERE error IS NULL\n",
    "  GROUP BY experiment_name, model_id\n",
    ")\n",
    "SELECT \n",
    "  *,\n",
    "  DATETIME_DIFF(last_run, first_run, MINUTE) as duration_minutes,\n",
    "  sample_count / DATETIME_DIFF(last_run, first_run, MINUTE) as runs_per_minute\n",
    "FROM experiment_stats\n",
    "ORDER BY avg_confidence DESC\n",
    "\"\"\"\n",
    "\n",
    "print(\"Running advanced analytics query...\")\n",
    "analytics_results = bq_client.query(analytics_query)\n",
    "analytics_df = pd.DataFrame([dict(row) for row in analytics_results])\n",
    "\n",
    "if len(analytics_df) > 0:\n",
    "    print(\"Advanced Analytics Results:\")\n",
    "    display(analytics_df)\n",
    "    \n",
    "    # Confidence distribution analysis\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for idx, row in analytics_df.iterrows():\n",
    "        plt.errorbar(\n",
    "            x=idx, \n",
    "            y=row['avg_confidence'], \n",
    "            yerr=row['std_confidence'],\n",
    "            label=f\"{row['experiment_name']} ({row['model_id']})\",\n",
    "            capsize=5\n",
    "        )\n",
    "    \n",
    "    plt.xlabel('Experiment')\n",
    "    plt.ylabel('Transcript Confidence')\n",
    "    plt.title('Transcript Confidence with Standard Deviation')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data for advanced analytics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476462f0",
   "metadata": {},
   "source": [
    "## Data Export and Cleanup\n",
    "\n",
    "Export data and demonstrate cleanup operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d2c61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data to CSV\n",
    "export_query = f\"\"\"\n",
    "SELECT *\n",
    "FROM `{PROJECT_ID}.{DATASET_NAME}.{TABLE_NAME}`\n",
    "WHERE error IS NULL\n",
    "ORDER BY timestamp DESC\n",
    "\"\"\"\n",
    "\n",
    "print(\"Exporting all experiment data...\")\n",
    "export_results = bq_client.query(export_query)\n",
    "export_df = pd.DataFrame([dict(row) for row in export_results])\n",
    "\n",
    "if len(export_df) > 0:\n",
    "    # Save to CSV\n",
    "    export_filename = f\"experiment_results_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    export_df.to_csv(export_filename, index=False)\n",
    "    print(f\"Exported {len(export_df)} rows to {export_filename}\")\n",
    "    \n",
    "    # Show summary\n",
    "    print(\"\\nExport Summary:\")\n",
    "    print(f\"Total experiments: {export_df['experiment_name'].nunique()}\")\n",
    "    print(f\"Total models: {export_df['model_id'].nunique()}\")\n",
    "    print(f\"Date range: {export_df['timestamp'].min()} to {export_df['timestamp'].max()}\")\n",
    "else:\n",
    "    print(\"No data to export\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39404b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup operations (optional)\n",
    "# Uncomment to actually run cleanup\n",
    "\n",
    "# Delete old test data (older than 7 days)\n",
    "cleanup_query = f\"\"\"\n",
    "DELETE FROM `{PROJECT_ID}.{DATASET_NAME}.{TABLE_NAME}`\n",
    "WHERE inserted_at < TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 7 DAY)\n",
    "  AND experiment_name LIKE '%test%'\n",
    "\"\"\"\n",
    "\n",
    "print(\"Cleanup query prepared (not executed):\")\n",
    "print(cleanup_query)\n",
    "\n",
    "# To actually run cleanup:\n",
    "# print(\"Running cleanup...\")\n",
    "# cleanup_results = bq_client.query(cleanup_query)\n",
    "# print(f\"Cleanup completed\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
