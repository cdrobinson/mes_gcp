{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8efc10bd",
   "metadata": {},
   "source": [
    "# BigQuery Operations for LLM Experiment Results\n",
    "\n",
    "This notebook demonstrates how to use the BigQuery client to create datasets, tables, and manage experiment results.\n",
    "\n",
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda6ac13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from google.cloud import bigquery\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add src directory to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "from clients.bigquery_client import BigQueryClient\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"BigQuery Operations notebook initialised successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7064a4f",
   "metadata": {},
   "source": [
    "## Initialise BigQuery Client\n",
    "\n",
    "Set up the BigQuery client with your project configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71017efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - Update these values for your project\n",
    "PROJECT_ID = \"gcp-project-id\"\n",
    "LOCATION = \"europe-west2\"\n",
    "DATASET_NAME = \"mes-llm_experiments\"\n",
    "TABLE_NAME = \"experiment_results\"\n",
    "\n",
    "# Initialise BigQuery client\n",
    "bq_client = BigQueryClient(\n",
    "    project_id=PROJECT_ID,\n",
    "    location=LOCATION\n",
    ")\n",
    "\n",
    "print(f\"BigQuery client initialised for project: {PROJECT_ID}\")\n",
    "print(f\"Location: {LOCATION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b476350e",
   "metadata": {},
   "source": [
    "## Create Dataset and Table\n",
    "\n",
    "Create the dataset and table structure for storing experiment results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91b814c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "def create_experiment_results_table(bq_client, dataset_name: str, table_name: str):\n",
    "    \"\"\"\n",
    "    Create a BigQuery table whose schema mirrors the LLM-evaluation\n",
    "    results DataFrame.  Field names with illegal characters (e.g. “/”)\n",
    "    are normalised -> '_' because BigQuery allows only letters,\n",
    "    numbers and underscores.\n",
    "\n",
    "    The table is partitioned by DATE(timestamp) and clustered on a few\n",
    "    high-cardinality dimensions that are handy for filtering.\n",
    "    \"\"\"\n",
    "    # --- 1.  Raw column list taken from the screenshot ----------\n",
    "    raw_cols = [\n",
    "        \"experiment_name\", \"model_id\", \"use_case\", \"audio_file\", \"prompt_id\", \"prompt\",\n",
    "        \"response\", \"reference\", \"metadata\", \"experiment_metrics\",\n",
    "        \"input_tokens\", \"output_tokens\", \"total_tokens\", \"processing_time\",\n",
    "        \"temperature\", \"top_k\", \"top_p\",\n",
    "        \"timestamp\",\n",
    "        \"summarisation_avg_log_probability\", \"summarisation_confidence\",\n",
    "        \"summarisation_section_coverage\", \"summarisation_required_sections\",\n",
    "        \"summarisation_section_length_compliance\", \"summarisation_format_empty\",\n",
    "        \"vertexai_groundedness/explanation\", \"vertexai_groundedness\", \"vertexai_summarizationquality/explanation\",\n",
    "        \"vertexai_summarizationquality\", \"vertexai_coherence/explanation\",\n",
    "        \"vertexai_coherence\", \"vertexai_safety/explanation\", \"vertexai_safety\",\n",
    "        \"vertexai_instructionfollowing\", \"vertexai_instructionfollowing/explanation\",\n",
    "        \"vertexai_fluency\", \"vertexai_fluency/explanation\",\n",
    "        \"vertexai_verbosity\", \"vertexai_verbosity/explanation\",\n",
    "    ]\n",
    "\n",
    "    # --- 2.  Map every column to a BigQuery type ----------------\n",
    "    # • Feel free to adjust if you know a column is INT64 vs FLOAT, etc.\n",
    "    column_type_map = {\n",
    "        # strings\n",
    "        \"STRING\": {\n",
    "            \"experiment_name\", \"model_id\", \"use_case\", \"audio_file\", \"prompt_id\", \"prompt\",\n",
    "            \"response\", \"reference\", \"metadata\", \"experiment_metrics\",\n",
    "            \"vertexai_groundedness/explanation\", \"vertexai_summarizationquality/explanation\", \"vertexai_coherence/explanation\",\n",
    "            \"vertexai_safety/explanation\", \"vertexai_instructionfollowing/explanation\",\n",
    "            \"vertexai_fluency/explanation\", \"vertexai_verbosity/explanation\",\n",
    "        },\n",
    "        # integers\n",
    "        \"INT64\": {\n",
    "            \"input_tokens\", \"output_tokens\", \"total_tokens\", \"top_k\",\n",
    "        },\n",
    "        # floats\n",
    "        \"FLOAT64\": {\n",
    "            \"processing_time\", \"temperature\", \"top_p\",\n",
    "            \"summarisation_avg_log_probability\",\n",
    "            \"summarisation_confidence\", \"summarisation_section_coverage\",\n",
    "            \"summarisation_required_sections\",\n",
    "            \"summarisation_section_length_compliance\",\n",
    "            \"summarisation_format_empty\",\n",
    "            \"vertexai_groundedness\", \"vertexai_summarizationquality\",\n",
    "            \"vertexai_coherence\", \"vertexai_safety\",\n",
    "            \"vertexai_instructionfollowing\", \"vertexai_fluency\",\n",
    "            \"vertexai_verbosity\",\n",
    "        },\n",
    "        # timestamps\n",
    "        \"TIMESTAMP\": {\"timestamp\"},\n",
    "    }\n",
    "\n",
    "    schema: list[bigquery.SchemaField] = []\n",
    "    for col in raw_cols:\n",
    "        # BigQuery field names can’t contain “/”\n",
    "        safe_col = col.replace(\"/\", \"_\")\n",
    "        # figure out the type\n",
    "        bq_type = next(typ for typ, cols in column_type_map.items() if col in cols)\n",
    "        schema.append(bigquery.SchemaField(safe_col, bq_type, mode=\"NULLABLE\"))\n",
    "\n",
    "    # --- 3.  Create table with partitioning + clustering ----------\n",
    "    time_partitioning = bigquery.TimePartitioning(\n",
    "        type_=bigquery.TimePartitioningType.DAY,\n",
    "        field=\"timestamp\",       # partition on DATE(timestamp)\n",
    "    )\n",
    "    clustering_fields = [\"experiment_name\", \"model_id\", \"use_case\"]\n",
    "\n",
    "    bq_client.create_table(\n",
    "        dataset_name=dataset_name,\n",
    "        table_name=table_name,\n",
    "        schema=schema,\n",
    "        time_partitioning=time_partitioning,\n",
    "        clustering_fields=clustering_fields,\n",
    "        exists_ok=True,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46265d8f",
   "metadata": {},
   "source": [
    "## Insert Sample Data\n",
    "\n",
    "Create and insert sample experiment data to test the BigQuery integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58f6d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample experiment data\n",
    "sample_data = {\n",
    "    'experiment_name': ['transcription_baseline', 'transcription_flash', 'transcription_baseline'],\n",
    "    'model_id': ['gemini-1.5-pro', 'gemini-1.5-flash', 'gemini-1.5-pro'],\n",
    "    'use_case': ['transcription', 'transcription', 'transcription'],\n",
    "    'audio_file': ['gs://bucket/audio1.wav', 'gs://bucket/audio2.wav', 'gs://bucket/audio3.wav'],\n",
    "    'prompt_id': ['transcription-prompt-v1', 'transcription-prompt-v1', 'transcription-prompt-v2'],\n",
    "    'response_text': ['Sample transcription 1', 'Sample transcription 2', 'Sample transcription 3'],\n",
    "    'metadata': ['{\"confidence\": 0.95}', '{\"confidence\": 0.87}', '{\"confidence\": 0.92}'],\n",
    "    'input_tokens': [150, 145, 148],\n",
    "    'output_tokens': [75, 82, 78],\n",
    "    'total_tokens': [225, 227, 226],\n",
    "    'processing_time': [2.34, 1.89, 2.12],\n",
    "    'temperature': [0.2, 0.2, 0.3],\n",
    "    'top_k': [40, 40, 32],\n",
    "    'top_p': [0.8, 0.8, 0.9],\n",
    "    'timestamp': [datetime.now(), datetime.now(), datetime.now()],\n",
    "    'run_timestamp': ['20241201_143022', '20241201_143022', '20241201_143022'],\n",
    "    'inserted_at': [datetime.now(), datetime.now(), datetime.now()],\n",
    "    'transcript_confidence': [0.95, 0.87, 0.92],\n",
    "    'transcript_format_compliance': [1.0, 0.95, 1.0],\n",
    "    'safety_overall': [0.15, 0.12, 0.18],\n",
    "    'safety_toxicity': [0.05, 0.03, 0.07]\n",
    "}\n",
    "\n",
    "sample_df = pd.DataFrame(sample_data)\n",
    "print(f\"Created sample data with {len(sample_df)} rows\")\n",
    "display(sample_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb08f8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert sample data into BigQuery\n",
    "try:\n",
    "    bq_client.insert_rows_from_dataframe(\n",
    "        dataset_name=DATASET_NAME,\n",
    "        table_name=TABLE_NAME,\n",
    "        df=sample_df,\n",
    "        ignore_unknown_values=True,\n",
    "        skip_invalid_rows=False\n",
    "    )\n",
    "    print(f\"Successfully inserted {len(sample_df)} rows into BigQuery\")\n",
    "except Exception as e:\n",
    "    print(f\"Error inserting data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b439b95c",
   "metadata": {},
   "source": [
    "## Query Experiment Results\n",
    "\n",
    "Demonstrate querying experiment results from BigQuery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b91af8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query all experiment results\n",
    "query = f\"\"\"\n",
    "SELECT \n",
    "    experiment_name,\n",
    "    model_id,\n",
    "    prompt_id,\n",
    "    temperature,\n",
    "    top_k,\n",
    "    top_p,\n",
    "    COUNT(*) as total_runs,\n",
    "    AVG(transcript_confidence) as avg_confidence,\n",
    "    AVG(safety_overall) as avg_safety,\n",
    "    AVG(processing_time) as avg_processing_time,\n",
    "    SUM(total_tokens) as total_tokens_used\n",
    "FROM `{PROJECT_ID}.{DATASET_NAME}.{TABLE_NAME}`\n",
    "WHERE error IS NULL\n",
    "GROUP BY experiment_name, model_id, prompt_id, temperature, top_k, top_p\n",
    "ORDER BY avg_confidence DESC\n",
    "\"\"\"\n",
    "\n",
    "print(\"Querying experiment summary...\")\n",
    "results = bq_client.query(query)\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "query_df = pd.DataFrame([dict(row) for row in results])\n",
    "print(f\"Query returned {len(query_df)} rows\")\n",
    "display(query_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2fc3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query recent experiments\n",
    "recent_query = f\"\"\"\n",
    "SELECT \n",
    "    experiment_name,\n",
    "    model_id,\n",
    "    audio_file,\n",
    "    transcript_confidence,\n",
    "    safety_overall,\n",
    "    processing_time,\n",
    "    timestamp\n",
    "FROM `{PROJECT_ID}.{DATASET_NAME}.{TABLE_NAME}`\n",
    "WHERE inserted_at >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 1 DAY)\n",
    "ORDER BY timestamp DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "print(\"Querying recent experiments...\")\n",
    "recent_results = bq_client.query(recent_query)\n",
    "recent_df = pd.DataFrame([dict(row) for row in recent_results])\n",
    "\n",
    "if len(recent_df) > 0:\n",
    "    print(f\"Found {len(recent_df)} recent experiments\")\n",
    "    display(recent_df)\n",
    "else:\n",
    "    print(\"No recent experiments found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf5627e",
   "metadata": {},
   "source": [
    "## Visualise BigQuery Results\n",
    "\n",
    "Create visualisations from BigQuery data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc548ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise model performance comparison\n",
    "if len(query_df) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Confidence by model\n",
    "    sns.barplot(data=query_df, x='model_id', y='avg_confidence', ax=axes[0,0])\n",
    "    axes[0,0].set_title('Average Transcript Confidence by Model')\n",
    "    axes[0,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Safety by model\n",
    "    sns.barplot(data=query_df, x='model_id', y='avg_safety', ax=axes[0,1])\n",
    "    axes[0,1].set_title('Average Safety Score by Model')\n",
    "    axes[0,1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Processing time by model\n",
    "    sns.barplot(data=query_df, x='model_id', y='avg_processing_time', ax=axes[1,0])\n",
    "    axes[1,0].set_title('Average Processing Time by Model')\n",
    "    axes[1,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Token usage by model\n",
    "    sns.barplot(data=query_df, x='model_id', y='total_tokens_used', ax=axes[1,1])\n",
    "    axes[1,1].set_title('Total Token Usage by Model')\n",
    "    axes[1,1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a64d906",
   "metadata": {},
   "source": [
    "## Advanced BigQuery Analytics\n",
    "\n",
    "Perform more complex analytics on experiment data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae13d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced analytics query\n",
    "analytics_query = f\"\"\"\n",
    "WITH experiment_stats AS (\n",
    "  SELECT \n",
    "    experiment_name,\n",
    "    model_id,\n",
    "    AVG(transcript_confidence) as avg_confidence,\n",
    "    STDDEV(transcript_confidence) as std_confidence,\n",
    "    AVG(safety_overall) as avg_safety,\n",
    "    AVG(processing_time) as avg_processing_time,\n",
    "    COUNT(*) as sample_count,\n",
    "    MIN(timestamp) as first_run,\n",
    "    MAX(timestamp) as last_run\n",
    "  FROM `{PROJECT_ID}.{DATASET_NAME}.{TABLE_NAME}`\n",
    "  WHERE error IS NULL\n",
    "  GROUP BY experiment_name, model_id\n",
    ")\n",
    "SELECT \n",
    "  *,\n",
    "  DATETIME_DIFF(last_run, first_run, MINUTE) as duration_minutes,\n",
    "  sample_count / DATETIME_DIFF(last_run, first_run, MINUTE) as runs_per_minute\n",
    "FROM experiment_stats\n",
    "ORDER BY avg_confidence DESC\n",
    "\"\"\"\n",
    "\n",
    "print(\"Running advanced analytics query...\")\n",
    "analytics_results = bq_client.query(analytics_query)\n",
    "analytics_df = pd.DataFrame([dict(row) for row in analytics_results])\n",
    "\n",
    "if len(analytics_df) > 0:\n",
    "    print(\"Advanced Analytics Results:\")\n",
    "    display(analytics_df)\n",
    "    \n",
    "    # Confidence distribution analysis\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for idx, row in analytics_df.iterrows():\n",
    "        plt.errorbar(\n",
    "            x=idx, \n",
    "            y=row['avg_confidence'], \n",
    "            yerr=row['std_confidence'],\n",
    "            label=f\"{row['experiment_name']} ({row['model_id']})\",\n",
    "            capsize=5\n",
    "        )\n",
    "    \n",
    "    plt.xlabel('Experiment')\n",
    "    plt.ylabel('Transcript Confidence')\n",
    "    plt.title('Transcript Confidence with Standard Deviation')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data for advanced analytics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab57ae5f",
   "metadata": {},
   "source": [
    "## Prompt and Generation Config Analysis with BigQuery\n",
    "\n",
    "Demonstrate how to analyze prompt versions and generation configuration impact using BigQuery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa36eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt performance comparison query\n",
    "prompt_analysis_query = f\"\"\"\n",
    "SELECT \n",
    "    prompt_id,\n",
    "    COUNT(*) as total_experiments,\n",
    "    COUNT(DISTINCT experiment_name) as unique_experiments,\n",
    "    AVG(transcript_confidence) as avg_confidence,\n",
    "    AVG(safety_overall) as avg_safety,\n",
    "    AVG(processing_time) as avg_processing_time,\n",
    "    STDDEV(transcript_confidence) as confidence_std\n",
    "FROM `{PROJECT_ID}.{DATASET_NAME}.{TABLE_NAME}`\n",
    "WHERE error IS NULL AND prompt_id IS NOT NULL\n",
    "GROUP BY prompt_id\n",
    "ORDER BY avg_confidence DESC\n",
    "\"\"\"\n",
    "\n",
    "print(\"Analyzing prompt performance...\")\n",
    "try:\n",
    "    prompt_results = bq_client.query(prompt_analysis_query)\n",
    "    prompt_df = pd.DataFrame([dict(row) for row in prompt_results])\n",
    "    \n",
    "    if len(prompt_df) > 0:\n",
    "        print(\"\\nPrompt Performance Analysis:\")\n",
    "        display(prompt_df.round(3))\n",
    "        \n",
    "        # Visualization\n",
    "        if len(prompt_df) > 1:\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "            \n",
    "            # Confidence comparison\n",
    "            sns.barplot(data=prompt_df, x='prompt_id', y='avg_confidence', ax=axes[0])\n",
    "            axes[0].set_title('Average Confidence by Prompt Version')\n",
    "            axes[0].tick_params(axis='x', rotation=45)\n",
    "            \n",
    "            # Processing time comparison\n",
    "            sns.barplot(data=prompt_df, x='prompt_id', y='avg_processing_time', ax=axes[1])\n",
    "            axes[1].set_title('Average Processing Time by Prompt Version')\n",
    "            axes[1].tick_params(axis='x', rotation=45)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"Only one prompt version found - no comparison possible\")\n",
    "    else:\n",
    "        print(\"No prompt data available for analysis\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error running prompt analysis: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dd520c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation configuration impact analysis\n",
    "config_analysis_query = f\"\"\"\n",
    "SELECT \n",
    "    temperature,\n",
    "    top_k,\n",
    "    top_p,\n",
    "    COUNT(*) as total_runs,\n",
    "    AVG(transcript_confidence) as avg_confidence,\n",
    "    AVG(safety_overall) as avg_safety,\n",
    "    AVG(processing_time) as avg_processing_time,\n",
    "    AVG(total_tokens) as avg_tokens\n",
    "FROM `{PROJECT_ID}.{DATASET_NAME}.{TABLE_NAME}`\n",
    "WHERE error IS NULL \n",
    "  AND temperature IS NOT NULL \n",
    "  AND top_k IS NOT NULL \n",
    "  AND top_p IS NOT NULL\n",
    "GROUP BY temperature, top_k, top_p\n",
    "ORDER BY avg_confidence DESC\n",
    "\"\"\"\n",
    "\n",
    "print(\"Analyzing generation configuration impact...\")\n",
    "try:\n",
    "    config_results = bq_client.query(config_analysis_query)\n",
    "    config_df = pd.DataFrame([dict(row) for row in config_results])\n",
    "    \n",
    "    if len(config_df) > 0:\n",
    "        print(\"\\nGeneration Configuration Analysis:\")\n",
    "        display(config_df.round(3))\n",
    "        \n",
    "        # Create a correlation analysis\n",
    "        if len(config_df) > 1:\n",
    "            print(\"\\nConfiguration Parameter Correlations with Performance:\")\n",
    "            corr_data = config_df[['temperature', 'top_k', 'top_p', 'avg_confidence', 'avg_processing_time']]\n",
    "            correlation_matrix = corr_data.corr()\n",
    "            \n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "            plt.title('Configuration Parameters vs Performance Correlation')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Temperature impact visualization\n",
    "            if len(config_df['temperature'].unique()) > 1:\n",
    "                plt.figure(figsize=(12, 5))\n",
    "                \n",
    "                plt.subplot(1, 2, 1)\n",
    "                sns.scatterplot(data=config_df, x='temperature', y='avg_confidence', size='total_runs')\n",
    "                plt.title('Temperature vs Confidence')\n",
    "                \n",
    "                plt.subplot(1, 2, 2)\n",
    "                sns.scatterplot(data=config_df, x='temperature', y='avg_processing_time', size='total_runs')\n",
    "                plt.title('Temperature vs Processing Time')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "        else:\n",
    "            print(\"Only one configuration found - no comparison possible\")\n",
    "    else:\n",
    "        print(\"No generation configuration data available for analysis\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error running config analysis: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476462f0",
   "metadata": {},
   "source": [
    "## Data Export and Cleanup\n",
    "\n",
    "Export data and demonstrate cleanup operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d2c61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data to CSV\n",
    "export_query = f\"\"\"\n",
    "SELECT *\n",
    "FROM `{PROJECT_ID}.{DATASET_NAME}.{TABLE_NAME}`\n",
    "WHERE error IS NULL\n",
    "ORDER BY timestamp DESC\n",
    "\"\"\"\n",
    "\n",
    "print(\"Exporting all experiment data...\")\n",
    "export_results = bq_client.query(export_query)\n",
    "export_df = pd.DataFrame([dict(row) for row in export_results])\n",
    "\n",
    "if len(export_df) > 0:\n",
    "    # Save to CSV\n",
    "    export_filename = f\"experiment_results_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    export_df.to_csv(export_filename, index=False)\n",
    "    print(f\"Exported {len(export_df)} rows to {export_filename}\")\n",
    "    \n",
    "    # Show summary\n",
    "    print(\"\\nExport Summary:\")\n",
    "    print(f\"Total experiments: {export_df['experiment_name'].nunique()}\")\n",
    "    print(f\"Total models: {export_df['model_id'].nunique()}\")\n",
    "    print(f\"Date range: {export_df['timestamp'].min()} to {export_df['timestamp'].max()}\")\n",
    "else:\n",
    "    print(\"No data to export\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39404b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup operations (optional)\n",
    "# Uncomment to actually run cleanup\n",
    "\n",
    "# Delete old test data (older than 7 days)\n",
    "cleanup_query = f\"\"\"\n",
    "DELETE FROM `{PROJECT_ID}.{DATASET_NAME}.{TABLE_NAME}`\n",
    "WHERE inserted_at < TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 7 DAY)\n",
    "  AND experiment_name LIKE '%test%'\n",
    "\"\"\"\n",
    "\n",
    "print(\"Cleanup query prepared (not executed):\")\n",
    "print(cleanup_query)\n",
    "\n",
    "# To actually run cleanup:\n",
    "# print(\"Running cleanup...\")\n",
    "# cleanup_results = bq_client.query(cleanup_query)\n",
    "# print(f\"Cleanup completed\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
