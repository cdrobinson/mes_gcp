{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38e4117f",
   "metadata": {},
   "source": [
    "# Summarisation Results Analysis\n",
    "\n",
    "This notebook analyses the results of summarisation experiments using the Model Evaluation Suite (MES) framework.\n",
    "\n",
    "## Features\n",
    "- **Weighted Scoring**: Combines multiple evaluation metrics with configurable weights\n",
    "- **Hard Failure Thresholds**: Auto-fails experiments below critical safety/groundedness thresholds  \n",
    "- **Performance Analysis**: Latency statistics and grade distribution\n",
    "- **Comparative Visualisation**: Charts comparing experiments across multiple dimensions\n",
    "\n",
    "## Scoring Methodology\n",
    "The scoring system uses a weighted combination of:\n",
    "- **VertexAI Groundedness** (25%) - Factual accuracy\n",
    "- **VertexAI Safety** (20%) - Safety compliance\n",
    "- **Summarisation Quality** (15%) - Framework's quality metric\n",
    "- **VertexAI Summarisation Quality** (10%) - VertexAI's assessment\n",
    "- **Other metrics** (30%) - Coherence, instruction following, fluency, verbosity\n",
    "\n",
    "Experiments failing safety or groundedness thresholds (< 0.5) are automatically graded 'F'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c73305",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add src directory to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "from clients.bigquery_client import BigQueryClient\n",
    "\n",
    "# Load configuration from YAML\n",
    "CONFIG_PATH = '../config/sample_experiments.yaml'\n",
    "\n",
    "with open(CONFIG_PATH, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"Configuration loaded successfully.\")\n",
    "print(f\"Project: {config['project']}\")\n",
    "print(f\"Location: {config['location']}\")\n",
    "print(f\"BigQuery Dataset: {config['bigquery']['dataset_name']}\")\n",
    "print(f\"BigQuery Table: {config['bigquery']['table_name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4893bf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise BigQuery client using framework\n",
    "bq_client = BigQueryClient(\n",
    "    project_id=config['bigquery']['project_id'],\n",
    "    location=config['bigquery']['location']\n",
    ")\n",
    "\n",
    "# Build table reference from config\n",
    "bq_dataset = config['bigquery']['dataset_name']\n",
    "bq_table = config['bigquery']['table_name']\n",
    "full_table_id = f\"{config['bigquery']['project_id']}.{bq_dataset}.{bq_table}\"\n",
    "\n",
    "print(f\"BigQuery client initialised for: {full_table_id}\")\n",
    "\n",
    "# Query experiment results - adjusted column names to match framework schema\n",
    "query = f\"\"\"\n",
    "SELECT\n",
    "  experiment_name,\n",
    "  model_id,\n",
    "  summarisation_quality,\n",
    "  vertexai_groundedness,\n",
    "  vertexai_summarization_quality,\n",
    "  vertexai_coherence, \n",
    "  vertexai_safety,\n",
    "  vertexai_instruction_following,\n",
    "  vertexai_fluency,\n",
    "  vertexai_verbosity,\n",
    "  latency_ms,\n",
    "  timestamp,\n",
    "  metadata\n",
    "FROM `{full_table_id}`\n",
    "WHERE use_case = 'summarisation'\n",
    "  AND error IS NULL\n",
    "\"\"\"\n",
    "\n",
    "print(\"Running query...\")\n",
    "results = bq_client.query(query)\n",
    "df_raw = pd.DataFrame([dict(row) for row in results])\n",
    "\n",
    "print(f\"Loaded {len(df_raw)} rows from BigQuery\")\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910b32d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoring weights configuration\n",
    "WEIGHTS = {\n",
    "    \"vertexai_groundedness\":        0.25,  # HIGH weight - factual accuracy\n",
    "    \"vertexai_safety\":              0.20,  # HIGH weight - safety compliance  \n",
    "    \"summarisation_quality\":        0.15,  # Framework's summarisation metric\n",
    "    \"vertexai_summarization_quality\": 0.10, # VertexAI's summarisation metric\n",
    "    \"vertexai_coherence\":           0.10,  # Logical flow and consistency\n",
    "    \"vertexai_instruction_following\": 0.10, # Following the prompt instructions\n",
    "    \"vertexai_fluency\":             0.06,  # Language quality\n",
    "    \"vertexai_verbosity\":           0.04,  # Appropriate length\n",
    "}\n",
    "\n",
    "# Hard failure thresholds - experiments scoring below these are auto-failed\n",
    "HARD_FAILS = {\n",
    "    \"vertexai_groundedness\":  0.50,   # Must be factually grounded\n",
    "    \"vertexai_safety\":        0.50,   # Must be safe\n",
    "}\n",
    "\n",
    "# Validate that we have the required columns\n",
    "required_cols = list(WEIGHTS.keys())\n",
    "missing_cols = [col for col in required_cols if col not in df_raw.columns]\n",
    "if missing_cols:\n",
    "    print(f\"WARNING: Missing columns in data: {missing_cols}\")\n",
    "    print(f\"Available columns: {list(df_raw.columns)}\")\n",
    "else:\n",
    "    print(\"‚úÖ All required metric columns are present\")\n",
    "\n",
    "print(f\"\\nWeights configuration:\")\n",
    "for metric, weight in WEIGHTS.items():\n",
    "    print(f\"  {metric}: {weight:.2f}\")\n",
    "print(f\"\\nTotal weight: {sum(WEIGHTS.values()):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6010dcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_score(row):\n",
    "    \"\"\"Calculate weighted score from available metrics\"\"\"\n",
    "    score = 0\n",
    "    total_weight = 0\n",
    "    \n",
    "    for metric, weight in WEIGHTS.items():\n",
    "        if metric in row and pd.notna(row[metric]):\n",
    "            score += row[metric] * weight\n",
    "            total_weight += weight\n",
    "        else:\n",
    "            print(f\"Warning: Missing or null value for {metric} in row\")\n",
    "    \n",
    "    # Normalise by actual total weight if some metrics are missing\n",
    "    return score / total_weight if total_weight > 0 else 0\n",
    "\n",
    "def letter_grade(score):\n",
    "    \"\"\"Convert numerical score to letter grade\"\"\"\n",
    "    if   score >= 0.90: return \"A\"\n",
    "    elif score >= 0.80: return \"B\" \n",
    "    elif score >= 0.70: return \"C\"\n",
    "    elif score >= 0.60: return \"D\"\n",
    "    else:               return \"F\"\n",
    "\n",
    "# Only proceed if we have data\n",
    "if len(df_raw) == 0:\n",
    "    print(\"‚ùå No data found. Check your query and table.\")\n",
    "else:\n",
    "    df = df_raw.copy()\n",
    "    \n",
    "    # Calculate weighted scores\n",
    "    df[\"weighted_score\"] = df.apply(weighted_score, axis=1)\n",
    "    \n",
    "    # Apply hard failure thresholds\n",
    "    for metric, threshold in HARD_FAILS.items():\n",
    "        if metric in df.columns:\n",
    "            failing_mask = df[metric] < threshold\n",
    "            num_failing = failing_mask.sum()\n",
    "            if num_failing > 0:\n",
    "                print(f\"‚ö†Ô∏è  {num_failing} experiments failed {metric} threshold ({threshold})\")\n",
    "                df.loc[failing_mask, \"weighted_score\"] = np.minimum(df[\"weighted_score\"], 0.59)\n",
    "    \n",
    "    # Assign letter grades\n",
    "    df[\"grade\"] = df[\"weighted_score\"].apply(letter_grade)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Processed {len(df)} experiments\")\n",
    "    print(f\"Grade distribution:\")\n",
    "    print(df[\"grade\"].value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c907cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latency analysis\n",
    "if 'latency_ms' in df.columns and df['latency_ms'].notna().any():\n",
    "    lat_stats = df[\"latency_ms\"].describe(percentiles=[0.5, 0.9, 0.99])[\n",
    "        [\"50%\", \"90%\", \"99%\", \"mean\", \"min\", \"max\"]\n",
    "    ]\n",
    "    print(\"üìä Latency Statistics (ms):\")\n",
    "    print(lat_stats)\n",
    "    \n",
    "    # Additional latency insights\n",
    "    print(f\"\\nüöÄ Performance Insights:\")\n",
    "    print(f\"Median latency: {df['latency_ms'].median():.0f}ms\")\n",
    "    print(f\"95th percentile: {df['latency_ms'].quantile(0.95):.0f}ms\")\n",
    "    print(f\"Experiments > 5s: {(df['latency_ms'] > 5000).sum()}/{len(df)}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No latency data available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813bb6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create experiment scoreboard\n",
    "if len(df) > 0:\n",
    "    # Group by experiment_name (framework uses this instead of experiment_id)\n",
    "    groupby_col = 'experiment_name' if 'experiment_name' in df.columns else 'model_id'\n",
    "    \n",
    "    scoreboard = (\n",
    "        df.groupby(groupby_col)\n",
    "          .agg(\n",
    "              samples          = (\"weighted_score\", \"size\"),\n",
    "              avg_score        = (\"weighted_score\", \"mean\"),\n",
    "              std_score        = (\"weighted_score\", \"std\"),\n",
    "              p50_latency_ms   = (\"latency_ms\", lambda s: np.percentile(s.dropna(), 50) if len(s.dropna()) > 0 else np.nan),\n",
    "              p90_latency_ms   = (\"latency_ms\", lambda s: np.percentile(s.dropna(), 90) if len(s.dropna()) > 0 else np.nan),\n",
    "              fail_rate        = (\"grade\", lambda g: (g == \"F\").mean()),\n",
    "              a_grade_rate     = (\"grade\", lambda g: (g == \"A\").mean()),\n",
    "          )\n",
    "          .sort_values(\"avg_score\", ascending=False)\n",
    "    )\n",
    "    \n",
    "    print(f\"üìà Experiment Scoreboard (Top 10):\")\n",
    "    print(\"=\" * 80)\n",
    "    display(scoreboard.head(10))\n",
    "    \n",
    "    # Summary insights\n",
    "    print(f\"\\nüéØ Key Insights:\")\n",
    "    print(f\"Best performing experiment: {scoreboard.index[0]} (score: {scoreboard.iloc[0]['avg_score']:.3f})\")\n",
    "    print(f\"Worst performing experiment: {scoreboard.index[-1]} (score: {scoreboard.iloc[-1]['avg_score']:.3f})\")\n",
    "    print(f\"Average fail rate: {scoreboard['fail_rate'].mean():.1%}\")\n",
    "    print(f\"Experiments with A grades: {(scoreboard['a_grade_rate'] > 0).sum()}/{len(scoreboard)}\")\n",
    "else:\n",
    "    print(\"‚ùå No data to create scoreboard\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0037291",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Enhanced visualisations\n",
    "if len(df) > 0 and 'weighted_score' in df.columns:\n",
    "    \n",
    "    # Set up the plotting style\n",
    "    plt.style.use('default')\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('Summarisation Experiment Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Average Weighted Score per Experiment\n",
    "    ax1 = axes[0, 0]\n",
    "    scoreboard[\"avg_score\"].plot(kind=\"barh\", ax=ax1, color='skyblue')\n",
    "    ax1.set_title(\"Average Weighted Score per Experiment\")\n",
    "    ax1.set_xlabel(\"Weighted Score\")\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.invert_yaxis()\n",
    "    \n",
    "    # 2. Latency comparison (if available)\n",
    "    ax2 = axes[0, 1]\n",
    "    if 'latency_ms' in df.columns and df['latency_ms'].notna().any():\n",
    "        latency_cols = [\"p50_latency_ms\", \"p90_latency_ms\"]\n",
    "        available_cols = [col for col in latency_cols if col in scoreboard.columns]\n",
    "        if available_cols:\n",
    "            scoreboard[available_cols].plot(kind=\"barh\", ax=ax2, color=['lightgreen', 'orange'])\n",
    "            ax2.set_title(\"Latency Distribution (p50 / p90)\")\n",
    "            ax2.set_xlabel(\"Latency (ms)\")\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "            ax2.invert_yaxis()\n",
    "        else:\n",
    "            ax2.text(0.5, 0.5, 'No latency data', ha='center', va='center', transform=ax2.transAxes)\n",
    "    else:\n",
    "        ax2.text(0.5, 0.5, 'No latency data available', ha='center', va='center', transform=ax2.transAxes)\n",
    "    \n",
    "    # 3. Grade distribution\n",
    "    ax3 = axes[1, 0]\n",
    "    grade_counts = df[\"grade\"].value_counts().reindex(['A', 'B', 'C', 'D', 'F'], fill_value=0)\n",
    "    colors = ['green', 'lightgreen', 'yellow', 'orange', 'red']\n",
    "    grade_counts.plot(kind='bar', ax=ax3, color=colors[:len(grade_counts)])\n",
    "    ax3.set_title(\"Grade Distribution\")\n",
    "    ax3.set_ylabel(\"Number of Experiments\")\n",
    "    ax3.set_xlabel(\"Grade\")\n",
    "    ax3.tick_params(axis='x', rotation=0)\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Score vs Fail Rate\n",
    "    ax4 = axes[1, 1]\n",
    "    if len(scoreboard) > 1:\n",
    "        ax4.scatter(scoreboard['avg_score'], scoreboard['fail_rate'], \n",
    "                   s=scoreboard['samples']*20, alpha=0.6, color='purple')\n",
    "        ax4.set_xlabel('Average Score')\n",
    "        ax4.set_ylabel('Fail Rate')\n",
    "        ax4.set_title('Score vs Fail Rate\\n(bubble size = sample count)')\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "    else:\n",
    "        ax4.text(0.5, 0.5, 'Need >1 experiment\\nfor comparison', \n",
    "                ha='center', va='center', transform=ax4.transAxes)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No data available for visualisation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ca2924",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This analysis provides a comprehensive view of summarisation experiment performance using weighted scoring and multiple evaluation dimensions.\n",
    "\n",
    "### Key Takeaways\n",
    "1. **Quality Focus**: High weights on groundedness and safety ensure reliable outputs\n",
    "2. **Performance Monitoring**: Latency tracking helps optimise inference costs\n",
    "3. **Comparative Analysis**: Easy identification of best-performing configurations\n",
    "\n",
    "### Next Steps\n",
    "- **Experiment Optimisation**: Focus on improving low-scoring experiments\n",
    "- **Threshold Tuning**: Adjust weights based on use case requirements\n",
    "- **Cost Analysis**: Combine performance data with token usage for ROI analysis\n",
    "- **A/B Testing**: Use insights to design follow-up experiments"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
