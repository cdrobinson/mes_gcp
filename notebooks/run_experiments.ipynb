{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e093b6a6",
   "metadata": {},
   "source": [
    "# Transcription LLM Evaluation Suite\n",
    "\n",
    "This notebook demonstrates how to evaluate different LLM models for audio transcription tasks using the simplified MES framework.\n",
    "\n",
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbe7dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add src directory to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "from orchestrator.experiment_runner import ExperimentRunner\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Transcription LLM Evaluation Suite initialised successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a494e4b",
   "metadata": {},
   "source": [
    "## Initialise Experiment Runner\n",
    "\n",
    "Load the configuration and initialize the experiment runner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebc11e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the experiment runner with configuration\n",
    "config_path = '../config/sample_experiments.yaml'\n",
    "runner = ExperimentRunner(config_path)\n",
    "\n",
    "print(f\"Available metrics: {list(runner.available_metrics.keys())}\")\n",
    "print(f\"Configured experiments: {[exp['name'] for exp in runner.config['experiments']]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cfccd3",
   "metadata": {},
   "source": [
    "## Run Transcription Experiments\n",
    "\n",
    "Execute the configured transcription experiments on the audio dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1110dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all transcription experiments (this may take a while)\n",
    "# You can also specify specific experiments: runner.run_experiments(['transcription_baseline'])\n",
    "\n",
    "print(\"Starting transcription experiment run...\")\n",
    "results_df = runner.run_experiments()\n",
    "\n",
    "print(f\"\\nTranscription experiment run completed!\")\n",
    "print(f\"Total results: {len(results_df)}\")\n",
    "print(f\"Shape: {results_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac5f57e",
   "metadata": {},
   "source": [
    "## Transcription Experiment Summary\n",
    "\n",
    "View high-level statistics about the transcription experiment run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d049f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get transcription experiment summary\n",
    "summary = runner.get_experiment_summary()\n",
    "\n",
    "print(\"Transcription Experiment Summary:\")\n",
    "print(\"=\"*50)\n",
    "for key, value in summary.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"{key}: {value:.3f}\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d563e56a",
   "metadata": {},
   "source": [
    "## Transcription Results Exploration\n",
    "\n",
    "Explore the transcription results dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f441e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about the transcription results\n",
    "print(\"Transcription Results DataFrame Info:\")\n",
    "print(results_df.info())\n",
    "\n",
    "print(\"\\nFirst few rows:\")\n",
    "display(results_df.head())\n",
    "\n",
    "print(\"\\nExperiment breakdown:\")\n",
    "print(results_df['experiment_name'].value_counts())\n",
    "\n",
    "print(\"\\nModel performance comparison:\")\n",
    "print(results_df['model_id'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43d8cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any errors in processing\n",
    "error_mask = results_df['experiment_name'].str.contains('error', na=False)\n",
    "if error_mask.any():\n",
    "    print(f\"Found {error_mask.sum()} results with errors:\")\n",
    "    display(results_df[error_mask][['experiment_name', 'audio_file', 'error']])\n",
    "else:\n",
    "    print(\"No processing errors found!\")\n",
    "\n",
    "# Remove error rows for analysis\n",
    "clean_results = results_df[~error_mask].copy()\n",
    "print(f\"\\nClean transcription results: {len(clean_results)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8902a1",
   "metadata": {},
   "source": [
    "## Transcription Performance Analysis\n",
    "\n",
    "Analyze transcription model performance across different metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48293654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify transcription metric columns\n",
    "metric_columns = [col for col in clean_results.columns \n",
    "                 if col.startswith(('transcript_', 'safety_'))]\n",
    "\n",
    "print(f\"Identified {len(metric_columns)} transcription metric columns:\")\n",
    "for col in metric_columns:\n",
    "    print(f\"  {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec487f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcription model performance comparison\n",
    "if len(clean_results) > 0:\n",
    "    # Group by experiment and calculate mean metrics\n",
    "    metric_summary = clean_results.groupby(['experiment_name', 'model_id'])[metric_columns].mean()\n",
    "    \n",
    "    print(\"Transcription Metric Summary by Experiment:\")\n",
    "    display(metric_summary.round(3))\n",
    "    \n",
    "    # Show key transcription metrics\n",
    "    key_metrics = [col for col in metric_columns if any(x in col for x in ['confidence', 'format_compliance', 'safety_overall'])]\n",
    "    if key_metrics:\n",
    "        print(\"\\nKey Transcription Metrics:\")\n",
    "        display(clean_results.groupby('experiment_name')[key_metrics].mean().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841ced42",
   "metadata": {},
   "source": [
    "## Transcription Performance Visualizations\n",
    "\n",
    "Create visualizations to compare transcription model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdd975a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing time comparison for transcription models\n",
    "if 'processing_time' in clean_results.columns:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Processing time by experiment\n",
    "    sns.boxplot(data=clean_results, x='experiment_name', y='processing_time', ax=axes[0])\n",
    "    axes[0].set_title('Transcription Processing Time by Experiment')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Processing time by model\n",
    "    sns.boxplot(data=clean_results, x='model_id', y='processing_time', ax=axes[1])\n",
    "    axes[1].set_title('Transcription Processing Time by Model')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68729b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token usage analysis for transcription\n",
    "token_columns = [col for col in clean_results.columns if 'token' in col.lower()]\n",
    "\n",
    "if token_columns:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, col in enumerate(token_columns[:4]):\n",
    "        if i < len(axes):\n",
    "            sns.barplot(data=clean_results, x='experiment_name', y=col, ax=axes[i])\n",
    "            axes[i].set_title(f'Transcription {col} by Experiment')\n",
    "            axes[i].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No token usage data available for analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d893fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcription quality and safety metrics heatmap\n",
    "safety_metrics = [col for col in metric_columns if 'safety' in col]\n",
    "transcript_metrics = [col for col in metric_columns if 'transcript' in col]\n",
    "\n",
    "if safety_metrics:\n",
    "    # Safety metrics heatmap\n",
    "    safety_data = clean_results.groupby('experiment_name')[safety_metrics].mean()\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.heatmap(safety_data.T, annot=True, cmap='RdYlBu_r', center=0.5)\n",
    "    plt.title('Safety Metrics by Transcription Experiment')\n",
    "    plt.ylabel('Safety Metrics')\n",
    "    plt.xlabel('Experiment')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if transcript_metrics:\n",
    "    # Transcript quality metrics heatmap\n",
    "    quality_data = clean_results.groupby('experiment_name')[transcript_metrics].mean()\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(quality_data.T, annot=True, cmap='RdYlGn', center=0.5)\n",
    "    plt.title('Transcript Quality Metrics by Experiment')\n",
    "    plt.ylabel('Transcript Quality Metrics')\n",
    "    plt.xlabel('Experiment')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a729da2",
   "metadata": {},
   "source": [
    "## Interactive Transcription Visualizations\n",
    "\n",
    "Create interactive plots for transcription performance analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3741072a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive scatter plot of transcription metrics\n",
    "if len(clean_results) > 0:\n",
    "    # Find transcription-specific metrics to plot\n",
    "    x_metric = None\n",
    "    y_metric = None\n",
    "    \n",
    "    # Look for transcript confidence and safety metrics\n",
    "    for col in metric_columns:\n",
    "        if 'confidence' in col and x_metric is None:\n",
    "            x_metric = col\n",
    "        elif 'safety' in col and 'overall' in col and y_metric is None:\n",
    "            y_metric = col\n",
    "    \n",
    "    if x_metric and y_metric:\n",
    "        import plotly.express as px\n",
    "        fig = px.scatter(\n",
    "            clean_results, \n",
    "            x=x_metric, \n",
    "            y=y_metric,\n",
    "            color='experiment_name',\n",
    "            size='processing_time',\n",
    "            hover_data=['model_id', 'audio_file'],\n",
    "            title=f'Transcription: {x_metric} vs {y_metric}'\n",
    "        )\n",
    "        fig.show()\n",
    "    else:\n",
    "        print(\"Suitable transcription metrics for scatter plot not found\")\n",
    "        print(f\"Available metrics: {metric_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55d4806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcription performance radar chart\n",
    "if len(metric_columns) >= 3:\n",
    "    # Select key transcription metrics for radar chart\n",
    "    selected_metrics = metric_columns[:5]\n",
    "    \n",
    "    # Calculate mean scores by experiment\n",
    "    radar_data = clean_results.groupby('experiment_name')[selected_metrics].mean()\n",
    "    \n",
    "    import plotly.graph_objects as go\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    for experiment in radar_data.index:\n",
    "        fig.add_trace(go.Scatterpolar(\n",
    "            r=radar_data.loc[experiment].values,\n",
    "            theta=selected_metrics,\n",
    "            fill='toself',\n",
    "            name=experiment\n",
    "        ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        polar=dict(\n",
    "            radialaxis=dict(\n",
    "                visible=True,\n",
    "                range=[0, 1]\n",
    "            )\n",
    "        ),\n",
    "        showlegend=True,\n",
    "        title=\"Transcription Model Performance Comparison (Radar Chart)\"\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "else:\n",
    "    print(f\"Need at least 3 metrics for radar chart. Found {len(metric_columns)}: {metric_columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ee7a79",
   "metadata": {},
   "source": [
    "## Detailed Transcription Analysis\n",
    "\n",
    "Perform detailed analysis on transcription quality metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26927c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcription performance analysis\n",
    "transcription_results = clean_results[clean_results['use_case'] == 'transcription']\n",
    "\n",
    "print(f\"Transcription experiments: {len(transcription_results)}\")\n",
    "\n",
    "if len(transcription_results) > 0:\n",
    "    trans_metrics = [col for col in metric_columns if 'transcript' in col]\n",
    "    if trans_metrics:\n",
    "        print(\"\\nTranscription Quality Metrics Summary:\")\n",
    "        display(transcription_results.groupby('experiment_name')[trans_metrics].mean().round(3))\n",
    "    \n",
    "    safety_metrics = [col for col in metric_columns if 'safety' in col]\n",
    "    if safety_metrics:\n",
    "        print(\"\\nSafety Metrics Summary:\")\n",
    "        display(transcription_results.groupby('experiment_name')[safety_metrics].mean().round(3))\n",
    "    \n",
    "    # Model comparison\n",
    "    print(\"\\nModel Comparison:\")\n",
    "    model_comparison = transcription_results.groupby('model_id')[metric_columns].mean()\n",
    "    display(model_comparison.round(3))\n",
    "else:\n",
    "    print(\"No transcription results found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e8fc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical significance testing for transcription experiments\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "if len(clean_results['experiment_name'].unique()) >= 2:\n",
    "    experiments = clean_results['experiment_name'].unique()[:2]  # Compare first two experiments\n",
    "    \n",
    "    exp1_data = clean_results[clean_results['experiment_name'] == experiments[0]]\n",
    "    exp2_data = clean_results[clean_results['experiment_name'] == experiments[1]]\n",
    "    \n",
    "    print(f\"Statistical comparison: {experiments[0]} vs {experiments[1]}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Test transcription-specific metrics\n",
    "    test_metrics = metric_columns[:5]  # Test first 5 metrics\n",
    "    \n",
    "    for metric in test_metrics:\n",
    "        if metric in exp1_data.columns and metric in exp2_data.columns:\n",
    "            # Remove NaN values\n",
    "            exp1_values = exp1_data[metric].dropna()\n",
    "            exp2_values = exp2_data[metric].dropna()\n",
    "            \n",
    "            if len(exp1_values) > 1 and len(exp2_values) > 1:\n",
    "                statistic, p_value = ttest_ind(exp1_values, exp2_values)\n",
    "                \n",
    "                significance = \"***\" if p_value < 0.001 else \"**\" if p_value < 0.01 else \"*\" if p_value < 0.05 else \"\"\n",
    "                \n",
    "                print(f\"{metric:30} | t={statistic:6.3f} | p={p_value:6.3f} {significance}\")\n",
    "                print(f\"{'':30} | Mean1={exp1_values.mean():6.3f} | Mean2={exp2_values.mean():6.3f}\")\n",
    "                print(\"-\" * 60)\n",
    "else:\n",
    "    print(\"Need at least 2 experiments for statistical comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0b586b",
   "metadata": {},
   "source": [
    "## Export Transcription Results\n",
    "\n",
    "Export transcription results for further analysis or reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5c92ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transcription summary report\n",
    "report_data = {\n",
    "    'transcription_experiment_summary': runner.get_experiment_summary(),\n",
    "    'transcription_metric_averages': clean_results.groupby('experiment_name')[metric_columns].mean().to_dict(),\n",
    "    'processing_stats': {\n",
    "        'total_files_processed': len(clean_results),\n",
    "        'avg_processing_time': clean_results['processing_time'].mean(),\n",
    "        'total_processing_time': clean_results['processing_time'].sum()\n",
    "    },\n",
    "    'model_comparison': clean_results.groupby('model_id')[metric_columns].mean().to_dict()\n",
    "}\n",
    "\n",
    "# Save to local file\n",
    "import json\n",
    "with open('transcription_experiment_report.json', 'w') as f:\n",
    "    json.dump(report_data, f, indent=2, default=str)\n",
    "\n",
    "print(\"Transcription experiment report saved to 'transcription_experiment_report.json'\")\n",
    "\n",
    "# Also save the full results as CSV\n",
    "clean_results.to_csv('transcription_detailed_results.csv', index=False)\n",
    "print(\"Detailed transcription results saved to 'transcription_detailed_results.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a532a8b5",
   "metadata": {},
   "source": [
    "## Transcription Experiment Conclusions\n",
    "\n",
    "Summarize key findings from the transcription experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0b2db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Transcription Experiment Conclusions:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if len(clean_results) > 0:\n",
    "    # Best performing experiment by average metric score\n",
    "    if metric_columns:\n",
    "        avg_scores = clean_results.groupby('experiment_name')[metric_columns].mean().mean(axis=1)\n",
    "        best_experiment = avg_scores.idxmax()\n",
    "        print(f\"Best overall transcription performance: {best_experiment} (avg score: {avg_scores[best_experiment]:.3f})\")\n",
    "    \n",
    "    # Best model comparison\n",
    "    if len(clean_results['model_id'].unique()) > 1:\n",
    "        model_scores = clean_results.groupby('model_id')[metric_columns].mean().mean(axis=1)\n",
    "        best_model = model_scores.idxmax()\n",
    "        print(f\"Best performing model: {best_model} (avg score: {model_scores[best_model]:.3f})\")\n",
    "    \n",
    "    # Fastest experiment\n",
    "    if 'processing_time' in clean_results.columns:\n",
    "        avg_times = clean_results.groupby('experiment_name')['processing_time'].mean()\n",
    "        fastest_experiment = avg_times.idxmin()\n",
    "        print(f\"Fastest transcription processing: {fastest_experiment} (avg time: {avg_times[fastest_experiment]:.2f}s)\")\n",
    "    \n",
    "    # Safety analysis\n",
    "    safety_cols = [col for col in metric_columns if 'safety' in col and 'overall' in col]\n",
    "    if safety_cols:\n",
    "        safety_scores = clean_results.groupby('experiment_name')[safety_cols].mean().mean(axis=1)\n",
    "        safest_experiment = safety_scores.idxmin()  # Lower safety scores are better\n",
    "        print(f\"Safest transcription content: {safest_experiment} (avg safety score: {safety_scores[safest_experiment]:.3f})\")\n",
    "    \n",
    "    # Transcript quality analysis\n",
    "    quality_cols = [col for col in metric_columns if 'transcript_confidence' in col]\n",
    "    if quality_cols:\n",
    "        quality_scores = clean_results.groupby('experiment_name')[quality_cols].mean().mean(axis=1)\n",
    "        best_quality = quality_scores.idxmax()\n",
    "        print(f\"Highest transcript quality: {best_quality} (avg confidence: {quality_scores[best_quality]:.3f})\")\n",
    "    \n",
    "    print(f\"\\nTranscription experiments completed: {len(clean_results['experiment_name'].unique())}\")\n",
    "    print(f\"Total audio files processed: {len(clean_results['audio_file'].unique())}\")\n",
    "    print(f\"Total processing time: {clean_results['processing_time'].sum():.2f} seconds\")\n",
    "else:\n",
    "    print(\"No transcription results to analyze.\")\n",
    "\n",
    "print(\"\\nTranscription experiment run completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
