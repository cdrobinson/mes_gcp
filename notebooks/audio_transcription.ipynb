{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35eb3cc2",
   "metadata": {},
   "source": [
    "# Audio Transcription Pipeline\n",
    "\n",
    "This notebook transcribes all audio files in a GCS bucket using Gemini 2.5 Pro and uploads the transcripts to the same bucket under a 'transcripts' folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8063b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import concurrent.futures\n",
    "from threading import Lock\n",
    "import time\n",
    "\n",
    "# Add src directory to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "from clients.gcs_client import GCSClient\n",
    "from clients.gemini_client import GeminiClient\n",
    "from utils.prompt_manager import PromptManager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b1e55f",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Load the configuration and set up clients for GCS, Gemini, and Prompt Manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e295af4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURATION ---\n",
    "CONFIG_PATH = '../config/sample_experiments.yaml'\n",
    "\n",
    "# Load YAML config\n",
    "try:\n",
    "    with open(CONFIG_PATH, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    print(\"Configuration loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Configuration file not found at {CONFIG_PATH}\")\n",
    "    config = {}\n",
    "\n",
    "project_id = config.get('project')\n",
    "location = config.get('location')\n",
    "bucket_name = config.get('bucket')\n",
    "\n",
    "# Model configuration for Gemini 2.5 Pro\n",
    "MODEL_ID = \"gemini-2.0-flash-exp\"  # Using Gemini 2.0 Flash as it supports audio\n",
    "GENERATION_CONFIG = {\n",
    "    \"temperature\": 0.1,  # Low temperature for accurate transcription\n",
    "    \"top_p\": 0.8,\n",
    "    \"top_k\": 32,\n",
    "    \"max_output_tokens\": 8192,\n",
    "}\n",
    "\n",
    "print(f\"Project: {project_id}\")\n",
    "print(f\"Location: {location}\")\n",
    "print(f\"Bucket: {bucket_name}\")\n",
    "print(f\"Model: {MODEL_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2563357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize clients\n",
    "try:\n",
    "    gcs_client = GCSClient(bucket_name=bucket_name)\n",
    "    prompt_manager = PromptManager(project=project_id, location=location)\n",
    "    gemini_client = GeminiClient(\n",
    "        model_id=MODEL_ID, \n",
    "        config={'project_id': project_id, 'location': location}\n",
    "    )\n",
    "    print(\"✅ All clients initialized successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error initializing clients: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fd60fa",
   "metadata": {},
   "source": [
    "## Prompt Configuration\n",
    "\n",
    "Set your transcription prompt ID here. The prompt should instruct the model to transcribe the audio accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42260204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your transcription prompt ID\n",
    "TRANSCRIPTION_PROMPT_ID = \"your-transcription-prompt-id\"  # Replace with your actual prompt ID\n",
    "\n",
    "# Load the transcription prompt\n",
    "try:\n",
    "    transcription_prompt = prompt_manager.load(TRANSCRIPTION_PROMPT_ID)\n",
    "    print(\"✅ Transcription prompt loaded successfully:\")\n",
    "    print(f\"Prompt: {transcription_prompt}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading transcription prompt: {e}\")\n",
    "    print(\"Please ensure you have created a transcription prompt and set the correct ID above.\")\n",
    "    # Use a default prompt for demonstration\n",
    "    transcription_prompt = \"\"\"Please transcribe the following audio accurately. \n",
    "    Return only the transcription text without any additional commentary or formatting.\"\"\"\n",
    "    print(f\"Using default prompt: {transcription_prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9ad63a",
   "metadata": {},
   "source": [
    "## Audio File Discovery\n",
    "\n",
    "Find all audio files in the bucket that need to be transcribed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8d7984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define audio file patterns to search for\n",
    "audio_patterns = [\n",
    "    \"*.wav\",\n",
    "    \"*.mp3\", \n",
    "    \"*.m4a\",\n",
    "    \"*.flac\",\n",
    "    \"calls/*.wav\",\n",
    "    \"calls/*.mp3\",\n",
    "    \"recordings/*.wav\",\n",
    "    \"recordings/*.mp3\"\n",
    "]\n",
    "\n",
    "print(\"Searching for audio files...\")\n",
    "audio_files = gcs_client.list_audio_files(audio_patterns)\n",
    "\n",
    "print(f\"✅ Found {len(audio_files)} audio files\")\n",
    "\n",
    "if audio_files:\n",
    "    # Display first few files\n",
    "    df_audio = pd.DataFrame(audio_files[:10], columns=['Audio File URI'])\n",
    "    display(df_audio)\n",
    "    if len(audio_files) > 10:\n",
    "        print(f\"... and {len(audio_files) - 10} more files\")\n",
    "else:\n",
    "    print(\"No audio files found. Please check your bucket and patterns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d331f5",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "Define utility functions for processing audio files and handling transcriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef010d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mime_type_from_path(path: str) -> str:\n",
    "    \"\"\"Get MIME type from file path based on extension.\"\"\"\n",
    "    extension = path.lower().split('.')[-1]\n",
    "    mime_types = {\n",
    "        'wav': 'audio/wav',\n",
    "        'mp3': 'audio/mpeg',\n",
    "        'm4a': 'audio/mp4',\n",
    "        'flac': 'audio/flac',\n",
    "        'aac': 'audio/aac'\n",
    "    }\n",
    "    return mime_types.get(extension, 'audio/wav')\n",
    "\n",
    "def generate_transcript_path(audio_path: str) -> str:\n",
    "    \"\"\"Generate the transcript file path in the transcripts folder.\"\"\"\n",
    "    # Remove file extension and add .txt\n",
    "    path_without_ext = Path(audio_path).with_suffix('')\n",
    "    return f\"transcripts/{path_without_ext.name}_transcript.txt\"\n",
    "\n",
    "def check_transcript_exists(transcript_path: str) -> bool:\n",
    "    \"\"\"Check if transcript already exists in GCS to avoid reprocessing.\"\"\"\n",
    "    try:\n",
    "        gcs_client.get_file_metadata(transcript_path)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def transcribe_audio_file_optimized(gcs_uri: str, prompt: str, skip_existing: bool = True) -> dict:\n",
    "    \"\"\"Transcribe a single audio file with optimizations.\"\"\"\n",
    "    try:\n",
    "        # Parse GCS URI to get the path\n",
    "        bucket_name, gcs_path = gcs_client.parse_gcs_uri(gcs_uri)\n",
    "        \n",
    "        # Check if transcript already exists\n",
    "        transcript_path = generate_transcript_path(gcs_path)\n",
    "        if skip_existing and check_transcript_exists(transcript_path):\n",
    "            return {\n",
    "                'success': True,\n",
    "                'transcript': '[SKIPPED - Already exists]',\n",
    "                'metadata': {'skipped': True},\n",
    "                'audio_path': gcs_path,\n",
    "                'mime_type': 'skipped',\n",
    "                'error': None,\n",
    "                'transcript_path': transcript_path,\n",
    "                'saved_to_gcs': True\n",
    "            }\n",
    "        \n",
    "        # Download audio data\n",
    "        audio_data = gcs_client.download_bytes(gcs_path)\n",
    "        mime_type = get_mime_type_from_path(gcs_path)\n",
    "        \n",
    "        # Generate transcription using Gemini\n",
    "        result = gemini_client.generate_from_audio(\n",
    "            audio_data=audio_data,\n",
    "            prompt=prompt,\n",
    "            generation_config=GENERATION_CONFIG,\n",
    "            mime_type=mime_type\n",
    "        )\n",
    "        \n",
    "        transcript_text = result.get('response_text', '')\n",
    "        metadata = result.get('metadata', {})\n",
    "        \n",
    "        return {\n",
    "            'success': True,\n",
    "            'transcript': transcript_text,\n",
    "            'metadata': metadata,\n",
    "            'audio_path': gcs_path,\n",
    "            'mime_type': mime_type,\n",
    "            'error': None\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'success': False,\n",
    "            'transcript': '',\n",
    "            'metadata': {},\n",
    "            'audio_path': gcs_path if 'gcs_path' in locals() else gcs_uri,\n",
    "            'mime_type': '',\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "def save_transcript_to_gcs_optimized(transcript: str, transcript_path: str) -> bool:\n",
    "    \"\"\"Save transcript to GCS as plain text - optimized version.\"\"\"\n",
    "    try:\n",
    "        # Use GCS client's direct upload from string (if available)\n",
    "        # Create temporary local file with just the transcript text\n",
    "        import tempfile\n",
    "        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:\n",
    "            f.write(transcript)\n",
    "            temp_path = f.name\n",
    "        \n",
    "        try:\n",
    "            # Upload to GCS\n",
    "            gcs_client.upload_file(temp_path, transcript_path)\n",
    "            return True\n",
    "        finally:\n",
    "            # Clean up temp file\n",
    "            os.unlink(temp_path)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving transcript: {e}\")\n",
    "        return False\n",
    "\n",
    "def process_audio_batch(audio_files_batch: list, prompt: str, skip_existing: bool = True) -> list:\n",
    "    \"\"\"Process a batch of audio files.\"\"\"\n",
    "    results = []\n",
    "    for audio_file in audio_files_batch:\n",
    "        # Transcribe audio\n",
    "        result = transcribe_audio_file_optimized(audio_file, prompt, skip_existing)\n",
    "        \n",
    "        if result['success'] and not result['metadata'].get('skipped', False):\n",
    "            # Save transcript to GCS\n",
    "            transcript_path = generate_transcript_path(result['audio_path'])\n",
    "            if save_transcript_to_gcs_optimized(result['transcript'], transcript_path):\n",
    "                result['transcript_path'] = transcript_path\n",
    "                result['saved_to_gcs'] = True\n",
    "            else:\n",
    "                result['saved_to_gcs'] = False\n",
    "        elif result['metadata'].get('skipped', False):\n",
    "            result['saved_to_gcs'] = True  # Already exists\n",
    "        else:\n",
    "            result['saved_to_gcs'] = False\n",
    "            \n",
    "        results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"✅ Optimized helper functions defined successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859d0164",
   "metadata": {},
   "source": [
    "## Test Run - Process First 2 Files\n",
    "\n",
    "Let's test the transcription pipeline with the first 2 audio files to ensure everything works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9111546f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with first 2 files\n",
    "test_files = audio_files[:2] if len(audio_files) >= 2 else audio_files\n",
    "print(f\"🧪 Testing transcription with {len(test_files)} file(s)...\")\n",
    "\n",
    "test_results = []\n",
    "\n",
    "for i, audio_file in enumerate(test_files, 1):\n",
    "    print(f\"\\n--- Processing Test File {i}/{len(test_files)} ---\")\n",
    "    print(f\"File: {audio_file}\")\n",
    "    \n",
    "    # Transcribe the audio\n",
    "    result = transcribe_audio_file_optimized(audio_file, transcription_prompt)\n",
    "    \n",
    "    if result['success']:\n",
    "        if result['metadata'].get('skipped', False):\n",
    "            print(f\"⏭️  File already transcribed, skipping\")\n",
    "        else:\n",
    "            print(f\"✅ Transcription successful\")\n",
    "            print(f\"Transcript length: {len(result['transcript'])} characters\")\n",
    "            print(f\"Processing time: {result['metadata'].get('latency_seconds', 'N/A')} seconds\")\n",
    "            \n",
    "            # Show first 200 characters of transcript\n",
    "            preview = result['transcript'][:200] + \"...\" if len(result['transcript']) > 200 else result['transcript']\n",
    "            print(f\"Preview: {preview}\")\n",
    "        \n",
    "        # Generate transcript path\n",
    "        transcript_path = generate_transcript_path(result['audio_path'])\n",
    "        print(f\"Will save to: gs://{bucket_name}/{transcript_path}\")\n",
    "        \n",
    "        # Save transcript to GCS if not skipped\n",
    "        if not result['metadata'].get('skipped', False):\n",
    "            if save_transcript_to_gcs_optimized(result['transcript'], transcript_path):\n",
    "                print(\"✅ Transcript saved to GCS\")\n",
    "                result['transcript_path'] = transcript_path\n",
    "            else:\n",
    "                print(\"❌ Failed to save transcript to GCS\")\n",
    "        else:\n",
    "            result['transcript_path'] = transcript_path\n",
    "            \n",
    "    else:\n",
    "        print(f\"❌ Transcription failed: {result['error']}\")\n",
    "    \n",
    "    test_results.append(result)\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Summary of test results\n",
    "successful_tests = sum(1 for r in test_results if r['success'])\n",
    "print(f\"\\n🧪 Test Summary: {successful_tests}/{len(test_results)} files processed successfully\")\n",
    "\n",
    "if successful_tests > 0:\n",
    "    print(\"✅ Test run completed successfully! Ready to process all files.\")\n",
    "else:\n",
    "    print(\"❌ Test run failed. Please check the errors above before proceeding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cab2a64",
   "metadata": {},
   "source": [
    "## Full Transcription Pipeline (Optimized)\n",
    "\n",
    "Process all audio files in the bucket using parallel processing and other optimizations for speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1e4853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Configuration\n",
    "MAX_WORKERS = 5  # Number of parallel workers (adjust based on API rate limits)\n",
    "BATCH_SIZE = 10  # Files to process in each batch\n",
    "SKIP_EXISTING = True  # Skip files that already have transcripts\n",
    "\n",
    "print(f\"⚡ Performance Settings:\")\n",
    "print(f\"   Max parallel workers: {MAX_WORKERS}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   Skip existing transcripts: {SKIP_EXISTING}\")\n",
    "print(f\"   Total files to process: {len(audio_files)}\")\n",
    "\n",
    "# Check how many already exist\n",
    "if SKIP_EXISTING:\n",
    "    existing_count = 0\n",
    "    print(\"🔍 Checking for existing transcripts...\")\n",
    "    for audio_file in audio_files[:10]:  # Sample check\n",
    "        _, gcs_path = gcs_client.parse_gcs_uri(audio_file)\n",
    "        transcript_path = generate_transcript_path(gcs_path)\n",
    "        if check_transcript_exists(transcript_path):\n",
    "            existing_count += 1\n",
    "    \n",
    "    estimated_existing = (existing_count / min(10, len(audio_files))) * len(audio_files)\n",
    "    print(f\"📊 Estimated {estimated_existing:.0f} files already transcribed (will be skipped)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727d0b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized Parallel Processing Pipeline\n",
    "total_files = len(audio_files)\n",
    "print(f\"🚀 Starting OPTIMIZED transcription of {total_files} audio files...\")\n",
    "print(\"⚡ Using parallel processing for maximum speed!\")\n",
    "\n",
    "# Confirm before processing all files\n",
    "print(f\"⚠️  About to process {total_files} audio files with {MAX_WORKERS} parallel workers.\")\n",
    "print(\"This will be much faster but may incur API costs.\")\n",
    "print(\"\\nPress Enter to continue or Ctrl+C to cancel...\")\n",
    "input()\n",
    "\n",
    "# Initialize results tracking\n",
    "all_results = []\n",
    "successful_transcriptions = 0\n",
    "failed_transcriptions = 0\n",
    "skipped_transcriptions = 0\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Thread-safe counters\n",
    "results_lock = Lock()\n",
    "\n",
    "def update_counters(results_batch):\n",
    "    global successful_transcriptions, failed_transcriptions, skipped_transcriptions\n",
    "    with results_lock:\n",
    "        for result in results_batch:\n",
    "            if result['success']:\n",
    "                if result['metadata'].get('skipped', False):\n",
    "                    skipped_transcriptions += 1\n",
    "                else:\n",
    "                    successful_transcriptions += 1\n",
    "            else:\n",
    "                failed_transcriptions += 1\n",
    "\n",
    "# Create batches for processing\n",
    "audio_batches = [audio_files[i:i + BATCH_SIZE] for i in range(0, len(audio_files), BATCH_SIZE)]\n",
    "total_batches = len(audio_batches)\n",
    "\n",
    "print(f\"📦 Created {total_batches} batches of ~{BATCH_SIZE} files each\")\n",
    "\n",
    "# Process batches in parallel\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    # Submit all batch jobs\n",
    "    future_to_batch = {\n",
    "        executor.submit(process_audio_batch, batch, transcription_prompt, SKIP_EXISTING): i \n",
    "        for i, batch in enumerate(audio_batches)\n",
    "    }\n",
    "    \n",
    "    # Process completed batches with progress bar\n",
    "    with tqdm(total=total_batches, desc=\"Processing batches\") as pbar:\n",
    "        for future in concurrent.futures.as_completed(future_to_batch):\n",
    "            batch_idx = future_to_batch[future]\n",
    "            \n",
    "            try:\n",
    "                batch_results = future.result()\n",
    "                all_results.extend(batch_results)\n",
    "                update_counters(batch_results)\n",
    "                \n",
    "                # Update progress\n",
    "                pbar.set_postfix({\n",
    "                    'Success': successful_transcriptions,\n",
    "                    'Skipped': skipped_transcriptions, \n",
    "                    'Failed': failed_transcriptions\n",
    "                })\n",
    "                pbar.update(1)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\n❌ Error processing batch {batch_idx}: {e}\")\n",
    "                # Add failed results for this batch\n",
    "                batch_size = len(audio_batches[batch_idx])\n",
    "                failed_results = [{\n",
    "                    'success': False,\n",
    "                    'transcript': '',\n",
    "                    'metadata': {},\n",
    "                    'audio_path': f'batch_{batch_idx}_file_{i}',\n",
    "                    'mime_type': '',\n",
    "                    'error': str(e),\n",
    "                    'saved_to_gcs': False\n",
    "                } for i in range(batch_size)]\n",
    "                \n",
    "                all_results.extend(failed_results)\n",
    "                with results_lock:\n",
    "                    failed_transcriptions += batch_size\n",
    "                pbar.update(1)\n",
    "\n",
    "# Final summary\n",
    "end_time = datetime.now()\n",
    "total_time = end_time - start_time\n",
    "\n",
    "print(\"\\n🎉 OPTIMIZED TRANSCRIPTION PIPELINE COMPLETED!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"📊 FINAL SUMMARY:\")\n",
    "print(f\"   Total files processed: {len(all_results)}\")\n",
    "print(f\"   ✅ Successful transcriptions: {successful_transcriptions}\")\n",
    "print(f\"   ⏭️  Skipped (already existed): {skipped_transcriptions}\")\n",
    "print(f\"   ❌ Failed transcriptions: {failed_transcriptions}\")\n",
    "print(f\"   ⏱️  Total processing time: {total_time}\")\n",
    "print(f\"   📈 Success rate: {((successful_transcriptions + skipped_transcriptions)/len(all_results)*100):.1f}%\")\n",
    "\n",
    "if successful_transcriptions > 0:\n",
    "    avg_time = total_time.total_seconds() / successful_transcriptions\n",
    "    print(f\"   ⏱️  Average time per NEW transcription: {avg_time:.1f} seconds\")\n",
    "\n",
    "print(f\"\\n⚡ Speed improvement: ~{MAX_WORKERS}x faster with parallel processing!\")\n",
    "print(f\"💰 Cost savings: Skipped {skipped_transcriptions} already-processed files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a714ee18",
   "metadata": {},
   "source": [
    "## Legacy Sequential Pipeline (Slower)\n",
    "\n",
    "This is the original sequential processing method. Use only if you encounter issues with the parallel version above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed910acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEGACY SEQUENTIAL PROCESSING (Use only if parallel version fails)\n",
    "# Confirm before processing all files\n",
    "total_files = len(audio_files)\n",
    "print(f\"⚠️  About to process {total_files} audio files SEQUENTIALLY (SLOW).\")\n",
    "print(\"Consider using the parallel version above for much faster processing.\")\n",
    "print(\"This may take a significant amount of time and will incur API costs.\")\n",
    "print(\"\\nPress Enter to continue or Ctrl+C to cancel...\")\n",
    "input()\n",
    "\n",
    "print(f\"🐌 Starting SEQUENTIAL transcription of {total_files} audio files...\")\n",
    "\n",
    "# Initialize results tracking\n",
    "all_results = []\n",
    "successful_transcriptions = 0\n",
    "failed_transcriptions = 0\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Process all files with progress bar\n",
    "for i, audio_file in enumerate(tqdm(audio_files, desc=\"Transcribing audio files\"), 1):\n",
    "    try:\n",
    "        # Clear previous output and show current progress\n",
    "        clear_output(wait=True)\n",
    "        print(f\"🎵 Processing file {i}/{total_files}\")\n",
    "        print(f\"Current file: {audio_file}\")\n",
    "        print(f\"✅ Successful: {successful_transcriptions}\")\n",
    "        print(f\"❌ Failed: {failed_transcriptions}\")\n",
    "        \n",
    "        elapsed = datetime.now() - start_time\n",
    "        if i > 1:\n",
    "            avg_time_per_file = elapsed.total_seconds() / (i - 1)\n",
    "            remaining_files = total_files - i + 1\n",
    "            eta = remaining_files * avg_time_per_file\n",
    "            print(f\"⏱️  ETA: {eta/60:.1f} minutes\")\n",
    "        \n",
    "        # Transcribe the audio file\n",
    "        result = transcribe_audio_file_optimized(audio_file, transcription_prompt)\n",
    "        \n",
    "        if result['success']:\n",
    "            # Generate transcript path and save to GCS\n",
    "            transcript_path = generate_transcript_path(result['audio_path'])\n",
    "            \n",
    "            if not result['metadata'].get('skipped', False):\n",
    "                if save_transcript_to_gcs_optimized(result['transcript'], transcript_path):\n",
    "                    result['transcript_path'] = transcript_path\n",
    "                    result['saved_to_gcs'] = True\n",
    "                    successful_transcriptions += 1\n",
    "                else:\n",
    "                    result['saved_to_gcs'] = False\n",
    "                    failed_transcriptions += 1\n",
    "            else:\n",
    "                result['transcript_path'] = transcript_path\n",
    "                result['saved_to_gcs'] = True\n",
    "                successful_transcriptions += 1\n",
    "        else:\n",
    "            failed_transcriptions += 1\n",
    "        \n",
    "        all_results.append(result)\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n⏹️  Processing interrupted by user\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Unexpected error processing {audio_file}: {e}\")\n",
    "        failed_transcriptions += 1\n",
    "        all_results.append({\n",
    "            'success': False,\n",
    "            'transcript': '',\n",
    "            'metadata': {},\n",
    "            'audio_path': audio_file,\n",
    "            'mime_type': '',\n",
    "            'error': str(e),\n",
    "            'saved_to_gcs': False\n",
    "        })\n",
    "\n",
    "# Final summary\n",
    "end_time = datetime.now()\n",
    "total_time = end_time - start_time\n",
    "\n",
    "clear_output(wait=True)\n",
    "print(\"🎉 SEQUENTIAL TRANSCRIPTION PIPELINE COMPLETED!\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"📊 SUMMARY:\")\n",
    "print(f\"   Total files processed: {len(all_results)}\")\n",
    "print(f\"   ✅ Successful transcriptions: {successful_transcriptions}\")\n",
    "print(f\"   ❌ Failed transcriptions: {failed_transcriptions}\")\n",
    "print(f\"   ⏱️  Total processing time: {total_time}\")\n",
    "print(f\"   📈 Success rate: {(successful_transcriptions/len(all_results)*100):.1f}%\")\n",
    "\n",
    "if successful_transcriptions > 0:\n",
    "    avg_time = total_time.total_seconds() / successful_transcriptions\n",
    "    print(f\"   ⏱️  Average time per file: {avg_time:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4f8aea",
   "metadata": {},
   "source": [
    "## Results Analysis\n",
    "\n",
    "Analyze the transcription results and display detailed statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2268ea89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create detailed results DataFrame\n",
    "results_data = []\n",
    "\n",
    "for result in all_results:\n",
    "    results_data.append({\n",
    "        'Audio File': result.get('audio_path', 'Unknown'),\n",
    "        'Success': result['success'],\n",
    "        'Transcript Length': len(result['transcript']) if result['transcript'] else 0,\n",
    "        'Processing Time (s)': result.get('metadata', {}).get('latency_seconds', 0),\n",
    "        'Saved to GCS': result.get('saved_to_gcs', False),\n",
    "        'Error': result.get('error', '')\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(results_data)\n",
    "\n",
    "print(\"📋 DETAILED RESULTS:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Display successful transcriptions\n",
    "successful_df = df_results[df_results['Success'] == True]\n",
    "if len(successful_df) > 0:\n",
    "    print(f\"\\n✅ SUCCESSFUL TRANSCRIPTIONS ({len(successful_df)} files):\")\n",
    "    display(successful_df[['Audio File', 'Transcript Length', 'Processing Time (s)', 'Saved to GCS']])\n",
    "    \n",
    "    print(f\"\\n📊 STATISTICS:\")\n",
    "    print(f\"   Average transcript length: {successful_df['Transcript Length'].mean():.0f} characters\")\n",
    "    print(f\"   Average processing time: {successful_df['Processing Time (s)'].mean():.1f} seconds\")\n",
    "    print(f\"   Total characters transcribed: {successful_df['Transcript Length'].sum():,}\")\n",
    "\n",
    "# Display failed transcriptions\n",
    "failed_df = df_results[df_results['Success'] == False]\n",
    "if len(failed_df) > 0:\n",
    "    print(f\"\\n❌ FAILED TRANSCRIPTIONS ({len(failed_df)} files):\")\n",
    "    display(failed_df[['Audio File', 'Error']])\n",
    "\n",
    "# Save results to CSV for future reference\n",
    "results_filename = f\"transcription_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "df_results.to_csv(results_filename, index=False)\n",
    "print(f\"\\n💾 Results saved to: {results_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396683d5",
   "metadata": {},
   "source": [
    "## Verify Transcripts in GCS\n",
    "\n",
    "Check that the transcripts were successfully uploaded to the GCS bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0aeeb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List transcript files in GCS\n",
    "print(\"🔍 Checking transcripts in GCS bucket...\")\n",
    "\n",
    "try:\n",
    "    # List files in the transcripts folder\n",
    "    transcript_patterns = [\"transcripts/*.json\", \"transcripts/*.txt\"]\n",
    "    transcript_files = gcs_client.list_audio_files(transcript_patterns)\n",
    "    \n",
    "    print(f\"✅ Found {len(transcript_files)} transcript files in GCS:\")\n",
    "    \n",
    "    if transcript_files:\n",
    "        # Display transcript files\n",
    "        df_transcripts = pd.DataFrame(transcript_files, columns=['Transcript File URI'])\n",
    "        display(df_transcripts)\n",
    "        \n",
    "        # Sample a transcript file to verify content\n",
    "        if len(transcript_files) > 0:\n",
    "            print(f\"\\n📄 Sampling content from: {transcript_files[0]}\")\n",
    "            try:\n",
    "                bucket_name, sample_path = gcs_client.parse_gcs_uri(transcript_files[0])\n",
    "                sample_content = gcs_client.download_bytes(sample_path).decode('utf-8')\n",
    "                \n",
    "                # Display the raw transcript content\n",
    "                print(f\"Transcript preview: {sample_content[:300]}...\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error reading sample file: {e}\")\n",
    "    else:\n",
    "        print(\"⚠️  No transcript files found in the transcripts folder.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error checking transcripts in GCS: {e}\")\n",
    "\n",
    "print(f\"\\n🎯 Transcription pipeline completed!\")\n",
    "print(f\"📁 All transcripts are saved in: gs://{bucket_name}/transcripts/\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
