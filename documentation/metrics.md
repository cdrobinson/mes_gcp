# Metrics Documentation

This document provides an overview of the metrics used in this project.

## Classification Metrics
These metrics are used to evaluate the performance of classification models.

| Metric Name                        | Description                                                                      | Status        |
| ---------------------------------- | -------------------------------------------------------------------------------- | ------------- |
| `LabelProperties`                  | Calculates properties of a predicted label, such as if a label was provided and if it's in an allowed set. | Implemented   |
| `ResponseLengthChars`              | Measures the character length of the LLM response for classification.            | Implemented   |
| `PotentialExplanationPresence`     | Checks if the classification response potentially contains an explanation beyond just a simple label, based on word count. | Implemented   |

## Safety Metrics
These metrics are used to assess the safety of AI models, such as by measuring the presence of harmful content in model outputs.

| Metric Name | Description                                     | Status          |
| ----------- | ----------------------------------------------- | --------------- |
| Profanity   | Measures the presence of profane language.      | Not Implemented |
| Threat      | Measures the presence of threatening language.  | Not Implemented |
| Toxicity    | Measures the overall toxicity of the content. | Not Implemented |

## Summarisation Metrics
These metrics are used to evaluate the quality of text summaries generated by AI models.

| Metric Name                 | Description                                     | Status        |
| --------------------------- | ----------------------------------------------- | ------------- |
| `FleschReadingEaseScore`    | Calculates the Flesch Reading Ease score for the summary. | Implemented   |
| `GunningFogIndex`    | Calculates the Gunning Fog Index score for the summary. | Implemented   |

## LLM Judge Evaluator
This is a framework that uses a large language model (LLM) to evaluate the performance of other AI models, possibly by comparing their outputs to a reference or by assessing their overall quality based on certain criteria.

It can evaluate different aspects based on the task:

**For Summarisation Tasks:**

| Metric Name  | Description                                                                                                | Status      |
|--------------|------------------------------------------------------------------------------------------------------------|-------------|
| Accuracy     | How factually correct is the summary, considering if it introduces information not present or contradicts the source. | Implemented |
| Completeness | How well does the summary capture the main points of the likely input.                                     | Implemented |
| Conciseness  | How well does the summary avoid unnecessary details while maintaining clarity.                             | Implemented |
| Coherence    | How logically structured and easy to follow is the summary.                                                | Implemented |
| Clarity      | How clear and understandable is the language used in the summary.                                          | Implemented |
| Overall      | An overall assessment of the summary quality.                                                              | Implemented |

**For Classification/Analysis Tasks:**

| Metric Name              | Description                                                                                             | Status      |
|--------------------------|---------------------------------------------------------------------------------------------------------|-------------|
| Appropriateness          | How appropriate is the analysis/classification given the likely input.                                  | Implemented |
| Confidence Justification | If a confidence score is provided, how well is it justified by the text (N/A if no confidence score). | Implemented |
| Clarity of Reasoning     | How clear is the reasoning behind the analysis/classification.                                          | Implemented |
| Potential Bias           | Does the analysis/classification show any signs of bias.                                                | Implemented |
| Helpfulness              | How helpful is this analysis/classification.                                                            | Implemented |
| Overall                  | An overall assessment of the analysis/classification quality.                                           | Implemented |

**For Q&A Groundedness (within Summarisation):**

| Metric Name          | Description                                                                          | Status      |
|----------------------|--------------------------------------------------------------------------------------|-------------|
| Groundedness Support | How well are answers to generated questions supported by the information in the summary. | Implemented |

## Ground Truth Dependent Metrics
These metrics can be calculated when ground truth (reference) data is available.

**For Summarisation Tasks (with Reference Summary):**

| Metric Name | Description                                                                                                | Status          |
|-------------|------------------------------------------------------------------------------------------------------------|-----------------|
| ROUGE (e.g., ROUGE-1, ROUGE-2, ROUGE-L) | Measures overlap of n-grams (ROUGE-1, ROUGE-2) or longest common subsequence (ROUGE-L) between generated and reference summary. | Not Implemented |
| BERTScore   | Calculates similarity between generated and reference summary using BERT embeddings.                       | Not Implemented |
| METEOR      | Measures unigram matching between generated and reference summary, considering stemming and synonymy.        | Not Implemented |

**For Classification Tasks (with True Labels):**

| Metric Name         | Description                                                                                                                   | Status          |
|---------------------|-------------------------------------------------------------------------------------------------------------------------------|-----------------|
| Accuracy            | Proportion of correctly classified instances.                                                                                 | Not Implemented |
| Precision           | Proportion of true positive predictions among all positive predictions (for a specific class).                                | Not Implemented |
| Recall (Sensitivity)| Proportion of true positive predictions among all actual positive instances (for a specific class).                             | Not Implemented |
| F1-Score            | Harmonic mean of precision and recall (for a specific class or averaged).                                                     | Not Implemented |
| Confusion Matrix    | Table showing the counts of true positive, true negative, false positive, and false negative predictions.                   | Not Implemented |
| AUC-ROC             | Area Under the Receiver Operating Characteristic curve; measures ability to distinguish between classes.                        | Not Implemented |

## Transcription Metrics
These metrics are used to evaluate the accuracy of speech-to-text transcription.

| Metric Name             | Description                                                                                                | Status          |
|-------------------------|------------------------------------------------------------------------------------------------------------|-----------------|
| Word Error Rate (WER)   | Measures the number of errors (substitutions, deletions, insertions) between a reference and a hypothesis transcript. | Not Implemented |
| Character Error Rate (CER) | Measures the number of character-level errors between a reference and a hypothesis transcript.             | Not Implemented |

## LLM Operational Metadata
This section lists key operational data points to record from the LLM during each call, which are crucial for evaluation, cost tracking, and performance analysis.

| Metadata Point         | Description                                                                                                | Status          |
|------------------------|------------------------------------------------------------------------------------------------------------|-----------------|
| Input Tokens           | Number of tokens in the input prompt provided to the LLM.                                                  | To Be Recorded  |
| Output Tokens          | Number of tokens in the response generated by the LLM.                                                     | To Be Recorded  |
| Total Tokens           | Sum of input and output tokens.                                                                            | To Be Recorded  |
| Processing Time (ms)   | Time taken by the LLM to generate the response, from request to receipt.                                   | To Be Recorded  |
| Model Name/ID          | Specific identifier of the LLM used (e.g., 'gemini-1.5-pro', 'gpt-4-turbo').                               | To Be Recorded  |
| Temperature            | Sampling temperature used for generation (controls randomness).                                              | To Be Recorded  |
| Top-K                  | Top-K sampling parameter used (if applicable).                                                             | To Be Recorded  |
| Top-P                  | Top-P (nucleus) sampling parameter used (if applicable).                                                     | To Be Recorded  |
| Max Output Tokens      | The maximum number of tokens configured for the LLM to generate.                                             | To Be Recorded  |
| API Call Cost          | Estimated or actual cost of the LLM API call, if available.                                                | To Be Recorded  |
| Retry Attempts         | Number of retry attempts made for the API call, if applicable.                                               | To Be Recorded  |
| Region/Endpoint        | The specific cloud region or endpoint the LLM was called from/to.                                          | To Be Recorded  |
