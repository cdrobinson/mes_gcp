"Now I’d like to show you another capability we’ve developed — one that Data Scientists will rely on throughout the entire lifecycle of a Gen AI agent."

"It’s called the Model Evaluation Suite, or MES for short. MES is a testing and evaluation tool designed to help Data Scientists make informed, data-driven decisions when developing Gen AI solutions."

"Before we even get to the point where the summarization agent is running in production — there’s a critical decision that needs to be made: selecting the right prompt and setup for the use case."

"That’s where MES comes in. MES allows Data Scientists to experiment with different prompt versions, model parameters, and configurations — and then evaluate the quality of responses using a suite of built-in metrics."

"Here, I’ve set up two experiments for our call summarization use case: one using a basic prompt, and another using a more advanced one."

(Run experiment – pause while results load or appear)

"What makes MES powerful is that it’s not just about trial and error — it’s guided experimentation. For every prompt or model setup tested, MES provides detailed evaluation results using both custom and pre-defined metrics — things like relevance, fluency, accuracy, and even domain-specific criteria."

"This enables teams to compare different versions of an agent in a consistent and objective way — and, more importantly, to understand why one version performs better than another."

"In this scenario, based on the generated responses and the metric scores, the Data Scientist would likely recommend using the advanced prompt — because it produces higher-quality responses, as shown by the MES evaluation."

"Once the right setup has been selected and deployed to production, it’s still vital to continue evaluating the agent’s outputs to ensure that responses remain safe, trustworthy, and aligned with expectations."

"Especially in a summarization use case like this, it's crucial that the agent doesn't introduce hallucinations — that is, information not actually present in the transcript or audio."

"Using MES, Data Scientists can evaluate live or batch responses from the summarization agent and apply built-in metrics to check for hallucinations or any potentially harmful content."

(Show evaluation results – highlight hallucination and safety metrics)

"As we can see here, the response passes all key safety checks — there are no hallucinations, no hate speech, and no harmful content."

"But if there were issues, MES makes it easy for the Data Scientist to drill down — to compare the output against the original transcript and pinpoint exactly where the problems occur."

"And that’s really the strength of MES: it supports Data Scientists across the entire lifecycle — from prompt design and experimentation, all the way through to post-deployment monitoring."

"That’s all I’ve got to show from MES today — a powerful way to help build Gen AI agents that are not only faster and more efficient, but also safer and more reliable."